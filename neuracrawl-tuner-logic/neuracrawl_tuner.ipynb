{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1fd78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import logging\n",
    "import sys\n",
    "from logging import getLogger\n",
    "import asyncio\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Logging\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "# Utilities\n",
    "\n",
    "\n",
    "def convert_url_to_file_name(url: str) -> str:\n",
    "    filename = url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "def batch_items[T](\n",
    "    items: list[T],\n",
    "    max_item_count_per_batch: int,\n",
    ") -> list[list[T]]:\n",
    "    batches = [\n",
    "        items[x_index : x_index + max_item_count_per_batch]\n",
    "        for x_index in range(0, len(items), max_item_count_per_batch)\n",
    "    ]\n",
    "    return batches\n",
    "\n",
    "\n",
    "## LLMs\n",
    "\n",
    "\n",
    "async def call_structured_llm[T: BaseModel](\n",
    "    system_prompt: str,\n",
    "    output_model: type[T],\n",
    ") -> T:\n",
    "    system_message = SystemMessage(system_prompt)\n",
    "    human_message = HumanMessage(\"Erledige die Aufgabe.\")\n",
    "    messages = [system_message, human_message]\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=\"http://127.0.0.1:50025\",\n",
    "        api_key=\"litellm-api-key-1234\",\n",
    "        model=\"bedrock/eu.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        timeout=120,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    structured_llm = llm.with_structured_output(\n",
    "        output_model,\n",
    "        method=\"json_schema\",\n",
    "    )\n",
    "    llm_output = await structured_llm.ainvoke(messages)\n",
    "    return llm_output\n",
    "\n",
    "\n",
    "MAX_CONCURRENT_BATCH_COUNT = 5\n",
    "\n",
    "\n",
    "async def call_structured_llm_batch[T: BaseModel](\n",
    "    system_prompts: list[str],\n",
    "    output_model: type[T],\n",
    ") -> list[T]:\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_BATCH_COUNT)\n",
    "\n",
    "    async def process_rate_limited(\n",
    "        batch_index: int,\n",
    "        system_prompt: str,\n",
    "    ) -> T:\n",
    "        async with semaphore:\n",
    "            log_message = (\n",
    "                f\"Processing prompt ({batch_index + 1}/{len(system_prompts)})...\"\n",
    "            )\n",
    "            logger.info(log_message)\n",
    "\n",
    "            result = await call_structured_llm(system_prompt, output_model)\n",
    "\n",
    "            log_message = f\"Processed prompt ({batch_index + 1}/{len(system_prompts)}).\"\n",
    "            logger.info(log_message)\n",
    "\n",
    "            return result\n",
    "\n",
    "    process_tasks = [\n",
    "        process_rate_limited(x_index, x_system_prompt)\n",
    "        for x_index, x_system_prompt in enumerate(system_prompts)\n",
    "    ]\n",
    "\n",
    "    all_results = await asyncio.gather(*process_tasks)\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# App comaptible save state\n",
    "\n",
    "\n",
    "class SaveStateDataPack(BaseModel):\n",
    "    url: str = Field()\n",
    "    raw_html: str = Field()\n",
    "    cleaned_html: str = Field()\n",
    "    raw_markdown: str = Field()\n",
    "    cleaned_markdown: str = Field()\n",
    "    feedback: str = Field()\n",
    "\n",
    "\n",
    "class SaveState(BaseModel):\n",
    "    versions_folder_path: Path = Field()\n",
    "    cloneable_result_folder_path: Path = Field()\n",
    "    data_packs: list[SaveStateDataPack] = Field()\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def get_current_versions(folder_path: Path) -> list[str]:\n",
    "    \"\"\"Get all existing version folders (e.g., ['v_000', 'v_001']).\"\"\"\n",
    "    versions = []\n",
    "    for item in folder_path.iterdir():\n",
    "        if item.is_dir() and item.name.startswith(\"v_\"):\n",
    "            versions.append(item.name)\n",
    "\n",
    "    return sorted(versions)\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def get_latest_version(folder_path: Path) -> str:\n",
    "    \"\"\"Get the latest version folder name.\"\"\"\n",
    "    versions = get_current_versions(folder_path)\n",
    "    if not versions:\n",
    "        return \"v_000\"\n",
    "    return versions[-1]\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def get_next_version(folder_path: Path) -> str:\n",
    "    \"\"\"Get the next version folder name.\"\"\"\n",
    "    latest = get_latest_version(folder_path)\n",
    "    if latest == \"v_000\" and not (folder_path / latest).exists():\n",
    "        return \"v_000\"\n",
    "\n",
    "    # Extract number from latest version (e.g., \"v_000\" -> 0)\n",
    "    version_num = int(latest.split(\"_\")[1])\n",
    "    next_num = version_num + 1\n",
    "    return f\"v_{next_num:03d}\"\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def save_save_state(save_state: SaveState) -> None:\n",
    "    \"\"\"Save the state to disk in the structured folder format.\"\"\"\n",
    "    # Create version folder\n",
    "    version_folder_path = save_state.versions_folder_path / get_next_version(\n",
    "        save_state.versions_folder_path\n",
    "    )\n",
    "    version_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Create results folder\n",
    "    results_folder_path = version_folder_path / \"results\"\n",
    "    results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Copy files from cloneable_result_folder_path to version's results folder for version history\n",
    "    if save_state.cloneable_result_folder_path.exists():\n",
    "        for item in save_state.cloneable_result_folder_path.iterdir():\n",
    "            if item.is_file():\n",
    "                shutil.copy2(item, results_folder_path / item.name)\n",
    "            elif item.is_dir():\n",
    "                shutil.copytree(\n",
    "                    item, results_folder_path / item.name, dirs_exist_ok=True\n",
    "                )\n",
    "\n",
    "    # Save each data pack\n",
    "    for idx, data_pack in enumerate(save_state.data_packs):\n",
    "        # Create sanitized folder name from URL using convert_url_to_file_name\n",
    "        folder_name = f\"{idx:03d}_{convert_url_to_file_name(data_pack.url)}\"\n",
    "\n",
    "        data_pack_folder_path = version_folder_path / folder_name\n",
    "        data_pack_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Save URL\n",
    "        (data_pack_folder_path / \"05_url.txt\").write_text(\n",
    "            data_pack.url, encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "        # Save raw HTML\n",
    "        (data_pack_folder_path / \"10_raw_html.html\").write_text(\n",
    "            data_pack.raw_html, encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "        # Save cleaned HTML\n",
    "        (data_pack_folder_path / \"20_cleaned_html.html\").write_text(\n",
    "            data_pack.cleaned_html, encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "        # Save raw markdown\n",
    "        (data_pack_folder_path / \"30_raw_markdown.md\").write_text(\n",
    "            data_pack.raw_markdown, encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "        # Save cleaned markdown\n",
    "        (data_pack_folder_path / \"40_cleaned_markdown.md\").write_text(\n",
    "            data_pack.cleaned_markdown, encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "        # Save feedback\n",
    "        (data_pack_folder_path / \"50_feedback.txt\").write_text(\n",
    "            data_pack.feedback, encoding=\"utf-8\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d5d3ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project\n",
    "\n",
    "project: str = \"\"\n",
    "\n",
    "\n",
    "def set_project(project_name) -> None:\n",
    "    global project\n",
    "    project = project_name\n",
    "\n",
    "\n",
    "def get_project() -> str:\n",
    "    global project\n",
    "    return project\n",
    "\n",
    "\n",
    "def get_project_dir_path() -> Path:\n",
    "    global project\n",
    "    project_dir_path = Path(os.getcwd(), \"projects\", project)\n",
    "    project_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    return project_dir_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "899a42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sitemap\n",
    "\n",
    "## Helpers\n",
    "\n",
    "### Sitemap URLS folder\n",
    "\n",
    "\n",
    "def get_sitemap_urls_folder_path() -> Path:\n",
    "    project_folder_path = get_project_dir_path()\n",
    "    folder_path = project_folder_path / \"05_sitemap_urls\"\n",
    "    folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return folder_path\n",
    "\n",
    "\n",
    "### Sitemap URLs file\n",
    "\n",
    "\n",
    "def get_sitemap_urls_txt_file_path() -> Path:\n",
    "    sitemap_urls_folder_path = get_sitemap_urls_folder_path()\n",
    "    file_path = sitemap_urls_folder_path / \"sitemap_urls.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_sitemap_urls(urls: list[str]) -> None:\n",
    "    urls_txt_file_path = get_sitemap_urls_txt_file_path()\n",
    "    urls_text = \"\\n\".join(urls)\n",
    "    with open(urls_txt_file_path, \"w\") as file:\n",
    "        file.write(urls_text)\n",
    "\n",
    "\n",
    "def get_saved_sitemap_urls() -> list[str]:\n",
    "    urls_txt_file_path = get_sitemap_urls_txt_file_path()\n",
    "    with open(urls_txt_file_path, \"r\") as file:\n",
    "        urls = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return urls\n",
    "\n",
    "\n",
    "### Frequent Sitemap URLs file\n",
    "\n",
    "\n",
    "def get_frequent_sitemap_urls_txt_file_path() -> Path:\n",
    "    sitemap_urls_folder_path = get_sitemap_urls_folder_path()\n",
    "    file_path = sitemap_urls_folder_path / \"frequent_sitemap_urls.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_frequent_sitemap_urls(frequent_urls_text: str) -> None:\n",
    "    frequent_urls_txt_file_path = get_frequent_sitemap_urls_txt_file_path()\n",
    "    with open(frequent_urls_txt_file_path, \"w\") as file:\n",
    "        file.write(frequent_urls_text)\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "def extract_sitemap_urls() -> None:\n",
    "    urls_txt_file_path = get_sitemap_urls_txt_file_path()\n",
    "    url_regex = r\"https?://[^\\s<>\\\"']+\"\n",
    "    with open(urls_txt_file_path, \"r\") as urls_txt_file:\n",
    "        text = urls_txt_file.read()\n",
    "    urls = re.findall(url_regex, text)\n",
    "    save_sitemap_urls(urls)\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def extract_frequent_sitemap_urls(min_frequency) -> None:\n",
    "    \"\"\"This function extracts common URL paths or path segments that appear frequently across URLs.\"\"\"\n",
    "    urls = get_saved_sitemap_urls()\n",
    "\n",
    "    # Extract path segments from each URL\n",
    "    path_segments = []\n",
    "\n",
    "    for x_url in urls:\n",
    "        # Remove protocol and domain\n",
    "        if \"://\" in x_url:\n",
    "            url_without_protocol = x_url.split(\"://\", 1)[1]\n",
    "            if \"/\" in url_without_protocol:\n",
    "                domain_and_path = url_without_protocol.split(\"/\", 1)\n",
    "                if len(domain_and_path) > 1:\n",
    "                    path = domain_and_path[1]\n",
    "                    # Build up all parent paths\n",
    "                    parts = path.rstrip(\"/\").split(\"/\")\n",
    "                    for i in range(1, len(parts) + 1):\n",
    "                        segment = \"/\".join(parts[:i]) + \"/\"\n",
    "                        path_segments.append(segment)\n",
    "\n",
    "    # Count frequency of each path segment\n",
    "    segment_counter = Counter(path_segments)\n",
    "\n",
    "    # Filter segments by minimum frequency\n",
    "    common_segments = {\n",
    "        segment: count\n",
    "        for segment, count in segment_counter.items()\n",
    "        if count >= min_frequency\n",
    "    }\n",
    "\n",
    "    # Sort by frequency (descending) and then alphabetically\n",
    "    sorted_segments = sorted(common_segments.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Write to output file\n",
    "    frequent_sitemap_urls_text = \"\\n\".join(\n",
    "        [f\"{x_count}\\t{x_segment}\" for x_segment, x_count in sorted_segments]\n",
    "    )\n",
    "    save_frequent_sitemap_urls(frequent_sitemap_urls_text)\n",
    "\n",
    "    print(\n",
    "        f\"Found {len(sorted_segments)} common URL areas with frequency >= {min_frequency}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fbee2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERAL_SYSTEM_PROMPT = \"\"\"<Generell>\n",
    "- neuracrawl\n",
    "    - neuracrawl ist ein Webcrawler, welcher eine Ausgangsdomain bekommt und von dort dann aus deepcrawlt, also sich durch alle Links der Seite hangelt und immer weiter nach neuen Links sucht.\n",
    "    - Er ist sehr gut darin, eine einzige Webseite sehr ausführlich zu crawlen.\n",
    "    - URL Ausschließungen\n",
    "        - Dabei schließt neuracrawl aber auch bestimmte URL Gruppen/Subpfade aus.\n",
    "        - Zum Beispiel, kann es sein, dass wir bei einer Webseite alle Veranstaltungen oder Newsartikel ausschließen wollen, da wir die zum Beispiel nochmal getrennt über eine API strukturiert auslesen.\n",
    "    - Markdown Extraktion\n",
    "        - Dabei extrahiert er extrem sauberes Markdown, ohne Header, Footer, Cookiebanner, Werbeinhalten, etc.\n",
    "        - Die Daten am Ende enthalten nur den reinen Inhalt der Webseite.\n",
    "        - Dabei geht er subtraktiv vor, also entfernt alle Elemente, welche \"Verschmutzungen\" darstellen.\n",
    "        - Dies ist immer ein Spiel zwischen \"wir wollen alles entfernen, was nicht wirklicher Inhalt ist\" und \"wir wollen nichts entfernen, was zum wirklichen Inhalt gehört\".\n",
    "        - Unser Grundsatz ist, dass wir so nah wie möglich an den wirklichen Inhalt rankommen wollen, ohne dabei aber Informationen zu verlieren. Wir dürfen auf keinen Fall echte Informationen verlieren, egal, wo diese auf der Webseite stehen.\n",
    "    - neuracrawl benötigt generell die folgenden Einstellungen:\n",
    "        - Ausgangsdomain und erlaubt andere Domains, auf welche er kommen und crawlen darf.\n",
    "        - URL-Ausschließ-Regexes, welche bestimmte URL Gruppen/Subpfade ausschließen\n",
    "            - Zum Beispiel : \"^.*/(aktuelles|amtsblatt)/.*$\" oder \"^.*\\\\.(?:ics|pdf).*$\"\n",
    "        - CSS-Ausschließ-Selektoren, welche bestimmte HTML-Elemente auf allen Seiten ausschließen\n",
    "            - Zum Beispiel : \"header\", \".front-left\" oder \"#cc-size\"\n",
    "\n",
    "- neuracrawl tuner ist eine Sammlung an Funktionen, welche dabei helfen, die perfekten Werte für die obigen Einstellungen zu finden.\n",
    "</Generell>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536eb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting URLs\n",
    "\n",
    "## Prompts\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_BATCH_SIZE = 500\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_COMMON_SYSTEM_PROMPT = \"\"\"<Prozess>\n",
    "- Du bist Teil des folgenden Prozesses:\n",
    "    - Um die CSS-Ausschließ-Selektoren zu bestimmen, müssen einige Sample Seiten der Webseite analysiert werden und auf diesen dann die CSS-Selektoren angewendet werden um zu schauen, ob sie den gewünschten Effekt haben.\n",
    "    - Dazu werden zu erst aus der Sitemap einer Webseite interessante, diverse URLs ausgewählt, welche das Sample Set darstellen.\n",
    "    - Dabei sollten diese Seiten besonders repräsentativ für die gesamte Webseite sein. Z. B. einmal die Startseite, dann eine Veranstaltungsseite, eine Newsseite, eine Übersichtsseite, eine Archivseite, eine Kontaktseite, eine Impressumseite, etc.\n",
    "    - Also ein Set an Seiten, bei dem wir auch unterschiedliche Inhalte und Layoutstrukturen erwarten.\n",
    "    - Natürlich können wir das nicht genau wissen, da wir nur die URLs sehen und aus diesen einfach von außen auswählen müssen. Tortzdem lässt sich an den URLs und Pfadsegmenten schon sehr gut ablesen, welche Seiten unterschiedliche Inhalte enthalten sollten.\n",
    "    - Da eine Webseite tausende Seiten enthalten kann, gehen wir hierbei in Batches vor. Zuerst extrahieren KI-Agenten aus jeweils 500 URLs ein Sample Set und begründen ihre Auswahlen.\n",
    "    - Dann nimmt ein zweiter KI-Agent die Batches und kombiniert diese zu einem finalen Sample Set.\n",
    "    - Das Sample Set wird dann später heruntergeladen und vom Nutzer analysiert.\n",
    "</Prozess>\"\"\"\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_BATCH_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welcher die Auswahl der URLs für das Sample Set vornimmt und dabei einen Batch von maximal 500 URLs bearbeitet. Du bist also nicht der, welcher am Ende die ganzen Batches zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<URLs>\n",
    "- Hier sind die URLs, aus welchen du ein Sample Set auswählen sollst:\n",
    "{urls_text}\n",
    "</URLs>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer die ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welche die ganzen Batches zu einem finalen Sample Set zusammenfasst.\n",
    "- Versuche maximal 20 URLs auszuwählen.\n",
    "</Aufgabe>\n",
    "\n",
    "<Batch Sample Sets>\n",
    "- Hier sind die Batches, welche du zusammenfassen sollst:\n",
    "{batch_llm_outputs_text}\n",
    "</Batch Sample Sets>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer die ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "## LLM Output models\n",
    "\n",
    "\n",
    "class InterestingUrlsExtractorUrlLlmOutput(BaseModel):\n",
    "    url: str = Field()\n",
    "    reason: str = Field()\n",
    "\n",
    "\n",
    "class InterestingUrlsExtractorLlmOutput(BaseModel):\n",
    "    urls: list[InterestingUrlsExtractorUrlLlmOutput] = Field()\n",
    "\n",
    "\n",
    "## System prompt methods\n",
    "\n",
    "\n",
    "def generate_interesting_urls_batch_system_prompt(\n",
    "    urls: list[str], custom_instructions: str\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(INTERESTING_URLS_EXTRACTION_COMMON_SYSTEM_PROMPT)\n",
    "\n",
    "    urls_text = \"\\n\".join(\n",
    "        [f\"{x_index + 1}. {x_url}\" for x_index, x_url in enumerate(urls)]\n",
    "    )\n",
    "    batch_system_prompt = INTERESTING_URLS_EXTRACTION_BATCH_SYSTEM_PROMPT.format(\n",
    "        urls_text=urls_text, custom_instructions=custom_instructions\n",
    "    )\n",
    "    parts.append(batch_system_prompt)\n",
    "\n",
    "    system_prompt = \"\\n\\n\".join(parts)\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "def generate_interesting_urls_summarizer_system_prompt(\n",
    "    batch_llm_outputs: list[InterestingUrlsExtractorLlmOutput],\n",
    "    custom_instructions: str,\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT)\n",
    "\n",
    "    batch_llm_outputs_text = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Batch {x_index + 1}:\\n\"\n",
    "            + \"\\n\".join(\n",
    "                [\n",
    "                    f\"- {url_output.url}\\n  Reason: {url_output.reason}\"\n",
    "                    for url_output in x_batch_llm_output.urls\n",
    "                ]\n",
    "            )\n",
    "            for x_index, x_batch_llm_output in enumerate(batch_llm_outputs)\n",
    "        ]\n",
    "    )\n",
    "    system_prompt = INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT.format(\n",
    "        batch_llm_outputs_text=batch_llm_outputs_text,\n",
    "        custom_instructions=custom_instructions,\n",
    "    )\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "## Helpers\n",
    "\n",
    "### Interesting URLs folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_folder_path() -> Path:\n",
    "    project_folder_path = get_project_dir_path()\n",
    "    file_path = project_folder_path / \"10_interesting_urls\"\n",
    "    file_path.mkdir(exist_ok=True, parents=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "### Results folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_results_folder_path() -> Path:\n",
    "    interesting_urls_folder_path = get_interesting_urls_folder_path()\n",
    "    results_folder_path = interesting_urls_folder_path / \"results\"\n",
    "    results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return results_folder_path\n",
    "\n",
    "\n",
    "### Interesting URLs file\n",
    "\n",
    "\n",
    "def save_interesting_urls(urls: list[str]) -> None:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    file_path = interesting_urls_results_folder_path / \"interesting_urls.txt\"\n",
    "    urls_text = \"\\n\".join(urls)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(urls_text)\n",
    "\n",
    "\n",
    "def get_interesting_urls() -> list[str]:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    interesting_urls_file_path = (\n",
    "        interesting_urls_results_folder_path / \"interesting_urls.txt\"\n",
    "    )\n",
    "    with open(interesting_urls_file_path, \"r\") as file:\n",
    "        urls = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def save_interesting_urls_reason(\n",
    "    llm_output: InterestingUrlsExtractorLlmOutput,\n",
    ") -> None:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    file_path = interesting_urls_results_folder_path / \"interesting_urls_reasons.txt\"\n",
    "    reason = llm_output.model_dump_json(indent=2)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(reason)\n",
    "\n",
    "\n",
    "### Interesting URLs downloads folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_downloads_folder_path() -> Path:\n",
    "    interesting_urls_folder_path = get_interesting_urls_folder_path()\n",
    "    downloaded_folder_path = interesting_urls_folder_path / \"downloads\"\n",
    "    downloaded_folder_path.mkdir(exist_ok=True)\n",
    "    return downloaded_folder_path\n",
    "\n",
    "\n",
    "### Interesting URLs downloads file\n",
    "\n",
    "\n",
    "def save_downloaded_url(index: int, url: str, html_content: str) -> None:\n",
    "    downloads_folder_path = get_interesting_urls_downloads_folder_path()\n",
    "    index_file_name_part = f\"{index:03d}\"\n",
    "    url_file_name_part = convert_url_to_file_name(url)\n",
    "    full_file_name = f\"{index_file_name_part}_{url_file_name_part}.html\"\n",
    "    file_path = downloads_folder_path / full_file_name\n",
    "    with open(file_path) as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "async def download_interesting_urls() -> None:\n",
    "    log_message = \"Downloading interesting URLs...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    urls = get_interesting_urls()\n",
    "\n",
    "    log_message = f\"Found {len(urls)} URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=60) as http_client:\n",
    "        download_tasks = [http_client.get(x_url) for x_url in urls]\n",
    "        responses = await asyncio.gather(*download_tasks)\n",
    "\n",
    "    data_packs: list[SaveStateDataPack] = []\n",
    "\n",
    "    for x_url, x_response in zip(urls, responses):\n",
    "        soup = BeautifulSoup(x_response.text, \"html.parser\")\n",
    "        prettified_html = soup.prettify()\n",
    "\n",
    "        data_pack = SaveStateDataPack(\n",
    "            url=x_url,\n",
    "            raw_html=prettified_html,\n",
    "            cleaned_html=\"\",\n",
    "            raw_markdown=\"\",\n",
    "            cleaned_markdown=\"\",\n",
    "            feedback=\"\",\n",
    "        )\n",
    "        data_packs.append(data_pack)\n",
    "\n",
    "    results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    downloads_folder_path = get_interesting_urls_downloads_folder_path()\n",
    "    save_state = SaveState(\n",
    "        versions_folder_path=downloads_folder_path,\n",
    "        cloneable_result_folder_path=results_folder_path,\n",
    "        data_packs=data_packs,\n",
    "    )\n",
    "    save_save_state(save_state)\n",
    "\n",
    "    log_message = f\"Downloaded {len(urls)} URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "\n",
    "async def extract_interesting_urls(custom_instructions: str) -> None:\n",
    "    log_message = \"Extracting interesting URLs...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    urls = get_saved_sitemap_urls()\n",
    "\n",
    "    log_message = f\"Found {len(urls)} URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    url_batches = batch_items(urls, INTERESTING_URLS_EXTRACTION_BATCH_SIZE)\n",
    "    batch_llm_outputs: list[InterestingUrlsExtractorLlmOutput] = []\n",
    "\n",
    "    batch_system_prompts = [\n",
    "        generate_interesting_urls_batch_system_prompt(x_url_batch, custom_instructions)\n",
    "        for x_url_batch in url_batches\n",
    "    ]\n",
    "\n",
    "    batch_llm_outputs = await call_structured_llm_batch(\n",
    "        batch_system_prompts,\n",
    "        InterestingUrlsExtractorLlmOutput,\n",
    "    )\n",
    "\n",
    "    log_message = f\"Summarizing {len(batch_llm_outputs)} batches...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    system_prompt = generate_interesting_urls_summarizer_system_prompt(\n",
    "        batch_llm_outputs, custom_instructions\n",
    "    )\n",
    "    llm_output = await call_structured_llm(\n",
    "        system_prompt,\n",
    "        InterestingUrlsExtractorLlmOutput,\n",
    "    )\n",
    "    urls = [x_url_output.url for x_url_output in llm_output.urls]\n",
    "\n",
    "    log_message = f\"Summarized {len(batch_llm_outputs)} batches and found {len(urls)} interesting URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    save_interesting_urls(urls)\n",
    "    save_interesting_urls_reason(llm_output)\n",
    "\n",
    "    log_message = f\"Extracted {len(urls)} interesting URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    await download_interesting_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "547231af",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Setup done.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSetup done.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Setup done."
     ]
    }
   ],
   "source": [
    "raise Exception(\"Setup done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd989b",
   "metadata": {},
   "source": [
    "# neuracrawl Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "526791d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your project name here\n",
    "\n",
    "set_project(\"siegburg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c957d",
   "metadata": {},
   "source": [
    "## Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "647e69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sitemap_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "13b3fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168 common URL areas with frequency >= 5\n"
     ]
    }
   ],
   "source": [
    "extract_frequent_sitemap_urls(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af9296",
   "metadata": {},
   "source": [
    "## Interesting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b9df22df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracting interesting URLs...\n",
      "INFO:__main__:Found 6883 URLs.\n",
      "INFO:__main__:Processing prompt (1/14)...\n",
      "INFO:__main__:Processing prompt (2/14)...\n",
      "INFO:__main__:Processing prompt (3/14)...\n",
      "INFO:__main__:Processing prompt (4/14)...\n",
      "INFO:__main__:Processing prompt (5/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (4/14).\n",
      "INFO:__main__:Processing prompt (6/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (2/14).\n",
      "INFO:__main__:Processing prompt (7/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (3/14).\n",
      "INFO:__main__:Processing prompt (8/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (5/14).\n",
      "INFO:__main__:Processing prompt (9/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (1/14).\n",
      "INFO:__main__:Processing prompt (10/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (6/14).\n",
      "INFO:__main__:Processing prompt (11/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (8/14).\n",
      "INFO:__main__:Processing prompt (12/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (9/14).\n",
      "INFO:__main__:Processing prompt (13/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (7/14).\n",
      "INFO:__main__:Processing prompt (14/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (10/14).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (11/14).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (12/14).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (13/14).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (14/14).\n",
      "INFO:__main__:Summarizing 14 batches...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Summarized 14 batches and found 39 interesting URLs.\n",
      "INFO:__main__:Extracted 39 interesting URLs.\n",
      "INFO:__main__:Downloading interesting URLs...\n",
      "INFO:__main__:Found 39 URLs.\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/stadtportraet/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/mitarbeiter/NRW:employee:62386/glathe-carsten/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/schulen/anno-gymnasium-siegburg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/datenschutz/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/newsletter/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/barrierefreiheit/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/umwelt-klimaschutz/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/wirtschaft-handel/ \"HTTP/1.1 301 301\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/wirtschaft-handel/stadtmarketing/siegburg-gutschein/ \"HTTP/1.1 301 301\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/serviceportal-a-z/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/newsletter/archiv/2024/juni/europawahl-2024-wahlnewsletter-nr-3/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/cityportal/einzelhandel/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/orte/sehenswuerdigkeiten/abtei-michaelsberg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/impressum/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/nachrichten/2024/april/siegburg-feiert/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/cityportal/firmen-unternehmen/absolut-designhaus/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/planen-bauen-verkehr/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/leistungen/NRW:entry:42373-VLR/terminreservierung-online/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/kultur-freizeit/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/cityportal/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/service-verwaltung/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/cityportal/gastronomiebetriebe/siegburger-brauhaus/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/umwelt-klimaschutz/klimaschutz-klimanpassung/buergergruen/buergergruen-foerderantrag/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/leistungen/NRW:entry:42049-VLR/personalausweis/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/leistungen/NRW:entry:40417-VLR/baugenehmigung-beantragen/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/stadtportraet/partnerstaedte/boleslawiec/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/feuerwehr-siegburg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/service-verwaltung/rathaus/stadtrat/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/service-verwaltung/rathaus/livestreams/ratssitzung/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/familie-bildung-soziales/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/newsletter/archiv/2024/juli/siegburgaktuell-15-07-2024/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/planen-bauen-verkehr/planen-bauen/bauvorhaben/rathaussanierung/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/abteilungen/NRW:department:15529/stadtverwaltung-siegburg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/nachrichten/2024/januar/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/sitemap/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/kalender/kombinierter-kalender/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/externe-datenquellen/siegburg-veranstaltungen/2226:0/herman-van-veen/ \"HTTP/1.1 200 200\"\n",
      "INFO:__main__:Downloaded 39 URLs.\n"
     ]
    }
   ],
   "source": [
    "await extract_interesting_urls(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1fc1b",
   "metadata": {},
   "source": [
    "## Exclusion CSS Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11406201",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1106298633.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mextract_exclusion_css_selectors(\"custom instructions\", based on what version?)\u001b[39m\n                                                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "extract_exclusion_css_selectors(\"custom instructions\", based on what version?)\n",
    "# insta apply\n",
    "# insta html to markdown\n",
    "# insta markdown clean\n",
    "# insta autofeedback, per document give markdown, html, cleaned html\n",
    "# insta file with css selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd36a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6106ed52",
   "metadata": {},
   "source": [
    "my viewer needs to be ok with urls file, and a folder with one folder with subfolders that are numbers, then /markdown.md, cleaned-html, raw-html, url.txt with one line beeing the url, + one file feeedback.txt. feedback txt should also be editable inside the swift app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844086a",
   "metadata": {},
   "source": [
    "need funcs to create new project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
