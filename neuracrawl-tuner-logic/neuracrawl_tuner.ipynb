{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from collections import Counter\n",
    "from logging import getLogger\n",
    "from pathlib import Path\n",
    "\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Logging\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "# Utilities\n",
    "\n",
    "\n",
    "def convert_url_to_file_name(url: str) -> str:\n",
    "    filename = url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "def batch_items[T](\n",
    "    items: list[T],\n",
    "    max_item_count_per_batch: int,\n",
    ") -> list[list[T]]:\n",
    "    batches = [\n",
    "        items[x_index : x_index + max_item_count_per_batch]\n",
    "        for x_index in range(0, len(items), max_item_count_per_batch)\n",
    "    ]\n",
    "    return batches\n",
    "\n",
    "\n",
    "## LLMs\n",
    "\n",
    "\n",
    "async def call_structured_llm[T: BaseModel](\n",
    "    system_prompt: str,\n",
    "    output_model: type[T],\n",
    ") -> T:\n",
    "    system_message = SystemMessage(system_prompt)\n",
    "    human_message = HumanMessage(\"Erledige die Aufgabe.\")\n",
    "    messages = [system_message, human_message]\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=\"http://127.0.0.1:50025\",\n",
    "        api_key=\"litellm-api-key-1234\",\n",
    "        # model=\"bedrock/eu.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        model=\"gemini/gemini-2.5-pro\",\n",
    "        timeout=120,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    structured_llm = llm.with_structured_output(\n",
    "        output_model,\n",
    "        method=\"json_schema\",\n",
    "    )\n",
    "    llm_output = await structured_llm.ainvoke(messages)\n",
    "    return llm_output\n",
    "\n",
    "\n",
    "MAX_CONCURRENT_BATCH_COUNT = 3\n",
    "\n",
    "\n",
    "async def call_structured_llm_batch[T: BaseModel](\n",
    "    system_prompts: list[str],\n",
    "    output_model: type[T],\n",
    ") -> list[T]:\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_BATCH_COUNT)\n",
    "\n",
    "    async def process_rate_limited(\n",
    "        batch_index: int,\n",
    "        system_prompt: str,\n",
    "    ) -> T:\n",
    "        async with semaphore:\n",
    "            log_message = (\n",
    "                f\"Processing prompt ({batch_index + 1}/{len(system_prompts)})...\"\n",
    "            )\n",
    "            logger.info(log_message)\n",
    "\n",
    "            result = await call_structured_llm(system_prompt, output_model)\n",
    "\n",
    "            log_message = f\"Processed prompt ({batch_index + 1}/{len(system_prompts)}).\"\n",
    "            logger.info(log_message)\n",
    "\n",
    "            return result\n",
    "\n",
    "    process_tasks = [\n",
    "        process_rate_limited(x_index, x_system_prompt)\n",
    "        for x_index, x_system_prompt in enumerate(system_prompts)\n",
    "    ]\n",
    "\n",
    "    all_results = await asyncio.gather(*process_tasks)\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Save state\n",
    "\n",
    "\n",
    "class SaveStateDataPack(BaseModel):\n",
    "    url: str = Field()  # The URL of the sample site\n",
    "    raw_html: str = Field()  # The raw downloaded HTML\n",
    "    cleaned_html: str = Field()  # Exclusion CSS selectors applied\n",
    "    raw_markdown: str = Field()  # The raw HTML converted to Markdown\n",
    "    cleaned_markdown: str = Field()  # The cleaned Markdown\n",
    "    feedback: str = Field()  # AI generated feedback for iterations\n",
    "\n",
    "\n",
    "class SaveState(BaseModel):\n",
    "    versions_folder_path: Path = Field()\n",
    "    cloneable_result_folder_path: Path = Field()\n",
    "    data_packs: list[SaveStateDataPack] = Field()\n",
    "\n",
    "\n",
    "def get_current_save_state_versions(folder_path: Path) -> list[str]:\n",
    "    versions: list[str] = []\n",
    "\n",
    "    for x_folder_path in folder_path.iterdir():\n",
    "        if x_folder_path.is_dir() and x_folder_path.name.startswith(\"v_\"):\n",
    "            versions.append(x_folder_path.name)\n",
    "\n",
    "    sorted_versions = sorted(versions)\n",
    "    return sorted_versions\n",
    "\n",
    "\n",
    "def get_latest_save_state_version(folder_path: Path) -> str:\n",
    "    versions = get_current_save_state_versions(folder_path)\n",
    "    if len(versions) == 0:\n",
    "        return \"v_000\"\n",
    "    latest_version = versions[-1]\n",
    "    return latest_version\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def get_next_save_state_version(folder_path: Path) -> str:\n",
    "    latest_version = get_latest_save_state_version(folder_path)\n",
    "    if latest_version == \"v_000\" and not (folder_path / latest_version).exists():\n",
    "        return \"v_000\"\n",
    "\n",
    "    # Extract number from latest version (e.g., \"v_000\" -> 0)\n",
    "    version_number = int(latest_version.split(\"_\")[1])\n",
    "    next_version_number = version_number + 1\n",
    "    next_version = f\"v_{next_version_number:03d}\"\n",
    "    return next_version\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def save_save_state(save_state: SaveState) -> None:\n",
    "    # Create version folder\n",
    "    next_version = get_next_save_state_version(save_state.versions_folder_path)\n",
    "    version_folder_path = save_state.versions_folder_path / next_version\n",
    "    version_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Save the save state as JSON\n",
    "    save_state_file_path = version_folder_path / \"save_state.json\"\n",
    "    save_state_json = save_state.model_dump_json(indent=2)\n",
    "    save_state_file_path.write_text(save_state_json)\n",
    "\n",
    "    # Create results folder\n",
    "    results_folder_path = version_folder_path / \"results\"\n",
    "    results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Copy files from cloneable_result_folder_path to version's results folder for version history\n",
    "    if save_state.cloneable_result_folder_path.exists():\n",
    "        for x_item in save_state.cloneable_result_folder_path.iterdir():\n",
    "            if x_item.is_file():\n",
    "                shutil.copy2(x_item, results_folder_path / x_item.name)\n",
    "            elif x_item.is_dir():\n",
    "                shutil.copytree(\n",
    "                    x_item, results_folder_path / x_item.name, dirs_exist_ok=True\n",
    "                )\n",
    "\n",
    "    # Save each data pack\n",
    "    for x_index, x_data_pack in enumerate(save_state.data_packs):\n",
    "        # Create sanitized folder name from URL using convert_url_to_file_name\n",
    "        data_pack_folder_name = (\n",
    "            f\"{x_index:03d}_{convert_url_to_file_name(x_data_pack.url)}\"\n",
    "        )\n",
    "        data_pack_folder_path = version_folder_path / data_pack_folder_name\n",
    "        data_pack_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Save URL\n",
    "        url_file_path = data_pack_folder_path / \"05_url.txt\"\n",
    "        url_file_path.write_text(x_data_pack.url)\n",
    "\n",
    "        # Save raw HTML\n",
    "        raw_html_file_path = data_pack_folder_path / \"10_raw_html.html\"\n",
    "        raw_html_file_path.write_text(x_data_pack.raw_html)\n",
    "\n",
    "        # Save cleaned HTML\n",
    "        cleaned_html_file_path = data_pack_folder_path / \"20_cleaned_html.html\"\n",
    "        cleaned_html_file_path.write_text(x_data_pack.cleaned_html)\n",
    "\n",
    "        # Save raw markdown\n",
    "        raw_markdown_file_path = data_pack_folder_path / \"30_raw_markdown.md\"\n",
    "        raw_markdown_file_path.write_text(x_data_pack.raw_markdown)\n",
    "\n",
    "        # Save cleaned markdown\n",
    "        cleaned_markdown_file_path = data_pack_folder_path / \"40_cleaned_markdown.md\"\n",
    "        cleaned_markdown_file_path.write_text(x_data_pack.cleaned_markdown)\n",
    "\n",
    "        # Save feedback\n",
    "        feedback_file_path = data_pack_folder_path / \"50_feedback.txt\"\n",
    "        feedback_file_path.write_text(x_data_pack.feedback)\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def get_save_state(versions_folder_path: Path, version: str | None = None) -> SaveState:\n",
    "    \"\"\"\n",
    "    Load a SaveState from a specific version or the latest version.\n",
    "\n",
    "    Args:\n",
    "        versions_folder_path: Path to the versions folder\n",
    "        version: Version string (e.g., \"v001\") or None to get the latest version\n",
    "\n",
    "    Returns:\n",
    "        SaveState object loaded from the specified or latest version\n",
    "    \"\"\"\n",
    "    if version is None:\n",
    "        # Get the latest version by finding the highest version number\n",
    "        version_folders = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in versions_folder_path.iterdir()\n",
    "                if d.is_dir() and d.name.startswith(\"v\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if not version_folders:\n",
    "            raise ValueError(f\"No version folders found in {versions_folder_path}\")\n",
    "\n",
    "        version_folder_path = version_folders[-1]\n",
    "    else:\n",
    "        version_folder_path = versions_folder_path / version\n",
    "\n",
    "        if not version_folder_path.exists():\n",
    "            raise ValueError(f\"Version folder {version_folder_path} does not exist\")\n",
    "\n",
    "    # Load the save state JSON file\n",
    "    save_state_file_path = version_folder_path / \"save_state.json\"\n",
    "\n",
    "    if not save_state_file_path.exists():\n",
    "        raise ValueError(f\"Save state file not found at {save_state_file_path}\")\n",
    "\n",
    "    save_state_json = save_state_file_path.read_text(encoding=\"utf-8\")\n",
    "    save_state = SaveState.model_validate_json(save_state_json)\n",
    "\n",
    "    return save_state\n",
    "\n",
    "\n",
    "# Converters\n",
    "\n",
    "\n",
    "def preclean_html(html: str) -> str:\n",
    "    pass\n",
    "\n",
    "\n",
    "def clean_html(html: str, exclusion_css_selectors: list[str]) -> str:\n",
    "    pass\n",
    "\n",
    "\n",
    "def convert_html_to_markdown(html: str) -> str:\n",
    "    pass\n",
    "\n",
    "\n",
    "def clean_markdown(markdown: str) -> str:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d5d3ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project\n",
    "\n",
    "project: str = \"\"\n",
    "\n",
    "\n",
    "def set_project(project_name) -> None:\n",
    "    global project\n",
    "    project = project_name\n",
    "\n",
    "\n",
    "def get_project() -> str:\n",
    "    global project\n",
    "    return project\n",
    "\n",
    "\n",
    "def get_project_dir_path() -> Path:\n",
    "    global project\n",
    "    project_dir_path = Path(os.getcwd(), \"projects\", project)\n",
    "    project_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    return project_dir_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "899a42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sitemap\n",
    "\n",
    "## File helpers\n",
    "\n",
    "### Sitemap URLS folder\n",
    "\n",
    "\n",
    "def get_sitemap_urls_folder_path() -> Path:\n",
    "    project_folder_path = get_project_dir_path()\n",
    "    folder_path = project_folder_path / \"05_sitemap_urls\"\n",
    "    folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return folder_path\n",
    "\n",
    "\n",
    "### Sitemap URLs file\n",
    "\n",
    "\n",
    "def get_sitemap_urls_txt_file_path() -> Path:\n",
    "    sitemap_urls_folder_path = get_sitemap_urls_folder_path()\n",
    "    file_path = sitemap_urls_folder_path / \"sitemap_urls.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_sitemap_urls(urls: list[str]) -> None:\n",
    "    urls_txt_file_path = get_sitemap_urls_txt_file_path()\n",
    "    urls_text = \"\\n\".join(urls)\n",
    "    with open(urls_txt_file_path, \"w\") as file:\n",
    "        file.write(urls_text)\n",
    "\n",
    "\n",
    "def get_saved_sitemap_urls() -> list[str]:\n",
    "    urls_txt_file_path = get_sitemap_urls_txt_file_path()\n",
    "    with open(urls_txt_file_path, \"r\") as file:\n",
    "        urls = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return urls\n",
    "\n",
    "\n",
    "### Frequent Sitemap URLs file\n",
    "\n",
    "\n",
    "def get_frequent_sitemap_urls_txt_file_path() -> Path:\n",
    "    sitemap_urls_folder_path = get_sitemap_urls_folder_path()\n",
    "    file_path = sitemap_urls_folder_path / \"frequent_sitemap_urls.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_frequent_sitemap_urls(frequent_urls_text: str) -> None:\n",
    "    frequent_urls_txt_file_path = get_frequent_sitemap_urls_txt_file_path()\n",
    "    with open(frequent_urls_txt_file_path, \"w\") as file:\n",
    "        file.write(frequent_urls_text)\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "def extract_sitemap_urls() -> None:\n",
    "    urls_txt_file_path = get_sitemap_urls_txt_file_path()\n",
    "    url_regex = r\"https?://[^\\s<>\\\"']+\"\n",
    "    with open(urls_txt_file_path, \"r\") as urls_txt_file:\n",
    "        text = urls_txt_file.read()\n",
    "    urls = re.findall(url_regex, text)\n",
    "    save_sitemap_urls(urls)\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def extract_frequent_sitemap_urls(min_frequency) -> None:\n",
    "    \"\"\"This function extracts common URL paths or path segments that appear frequently across URLs.\"\"\"\n",
    "    urls = get_saved_sitemap_urls()\n",
    "\n",
    "    # Extract path segments from each URL\n",
    "    path_segments = []\n",
    "\n",
    "    for x_url in urls:\n",
    "        # Remove protocol and domain\n",
    "        if \"://\" in x_url:\n",
    "            url_without_protocol = x_url.split(\"://\", 1)[1]\n",
    "            if \"/\" in url_without_protocol:\n",
    "                domain_and_path = url_without_protocol.split(\"/\", 1)\n",
    "                if len(domain_and_path) > 1:\n",
    "                    path = domain_and_path[1]\n",
    "                    # Build up all parent paths\n",
    "                    parts = path.rstrip(\"/\").split(\"/\")\n",
    "                    for i in range(1, len(parts) + 1):\n",
    "                        segment = \"/\".join(parts[:i]) + \"/\"\n",
    "                        path_segments.append(segment)\n",
    "\n",
    "    # Count frequency of each path segment\n",
    "    segment_counter = Counter(path_segments)\n",
    "\n",
    "    # Filter segments by minimum frequency\n",
    "    common_segments = {\n",
    "        segment: count\n",
    "        for segment, count in segment_counter.items()\n",
    "        if count >= min_frequency\n",
    "    }\n",
    "\n",
    "    # Sort by frequency (descending) and then alphabetically\n",
    "    sorted_segments = sorted(common_segments.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Write to output file\n",
    "    frequent_sitemap_urls_text = \"\\n\".join(\n",
    "        [f\"{x_count}\\t{x_segment}\" for x_segment, x_count in sorted_segments]\n",
    "    )\n",
    "    save_frequent_sitemap_urls(frequent_sitemap_urls_text)\n",
    "\n",
    "    print(\n",
    "        f\"Found {len(sorted_segments)} common URL areas with frequency >= {min_frequency}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fbee2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERAL_SYSTEM_PROMPT = \"\"\"<Generell>\n",
    "- neuracrawl\n",
    "    - neuracrawl ist ein Webcrawler, welcher eine Ausgangsdomain bekommt und von dort dann aus deepcrawlt, also sich durch alle Links der Seite hangelt und immer weiter nach neuen Links sucht.\n",
    "    - Er ist sehr gut darin, eine einzige Webseite sehr ausführlich zu crawlen.\n",
    "    - URL Ausschließungen\n",
    "        - Dabei schließt neuracrawl aber auch bestimmte URL Gruppen/Subpfade aus.\n",
    "        - Zum Beispiel, kann es sein, dass wir bei einer Webseite alle Veranstaltungen oder Newsartikel ausschließen wollen, da wir die zum Beispiel nochmal getrennt über eine API strukturiert auslesen.\n",
    "    - Markdown Extraktion\n",
    "        - Dabei extrahiert er extrem sauberes Markdown, ohne Header, Footer, Cookiebanner, Werbeinhalten, etc.\n",
    "        - Die Daten am Ende enthalten nur den reinen Inhalt der Webseite.\n",
    "        - Dabei geht er subtraktiv vor, also entfernt alle Elemente, welche \"Verschmutzungen\" darstellen.\n",
    "        - Dies ist immer ein Spiel zwischen \"wir wollen alles entfernen, was nicht wirklicher Inhalt ist\" und \"wir wollen nichts entfernen, was zum wirklichen Inhalt gehört\".\n",
    "        - Unser Grundsatz ist, dass wir so nah wie möglich an den wirklichen Inhalt rankommen wollen, ohne dabei aber Informationen zu verlieren. Wir dürfen auf keinen Fall echte Informationen verlieren, egal, wo diese auf der Webseite stehen.\n",
    "    - neuracrawl benötigt generell die folgenden Einstellungen:\n",
    "        - Ausgangsdomain und erlaubt andere Domains, auf welche er kommen und crawlen darf.\n",
    "        - URL-Ausschließ-Regexes, welche bestimmte URL Gruppen/Subpfade ausschließen\n",
    "            - Zum Beispiel : \"^.*/(aktuelles|amtsblatt)/.*$\" oder \"^.*\\\\.(?:ics|pdf).*$\"\n",
    "        - CSS-Ausschließ-Selektoren, welche bestimmte HTML-Elemente auf allen Seiten ausschließen\n",
    "            - Zum Beispiel : \"header\", \".front-left\" oder \"#cc-size\"\n",
    "\n",
    "- neuracrawl tuner ist eine Sammlung an Funktionen, welche dabei helfen, die perfekten Werte für die obigen Einstellungen zu finden.\n",
    "</Generell>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536eb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting URLs\n",
    "\n",
    "## Prompts\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_BATCH_SIZE = 500\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_COMMON_SYSTEM_PROMPT = \"\"\"<Prozess>\n",
    "- Du bist Teil des folgenden Prozesses:\n",
    "    - Um die CSS-Ausschließ-Selektoren zu bestimmen, müssen einige Sample Seiten der Webseite analysiert werden und auf diesen dann die CSS-Selektoren angewendet werden um zu schauen, ob sie den gewünschten Effekt haben.\n",
    "    - Dazu werden zu erst aus der Sitemap einer Webseite interessante, diverse URLs ausgewählt, welche das Sample Set darstellen.\n",
    "    - Dabei sollten diese Seiten besonders repräsentativ für die gesamte Webseite sein. Z. B. einmal die Startseite, dann eine Veranstaltungsseite, eine Newsseite, eine Übersichtsseite, eine Archivseite, eine Kontaktseite, eine Impressumseite, etc.\n",
    "    - Also ein Set an Seiten, bei dem wir auch unterschiedliche Inhalte und Layoutstrukturen erwarten.\n",
    "    - Natürlich können wir das nicht genau wissen, da wir nur die URLs sehen und aus diesen einfach von außen auswählen müssen. Tortzdem lässt sich an den URLs und Pfadsegmenten schon sehr gut ablesen, welche Seiten unterschiedliche Inhalte enthalten sollten.\n",
    "    - Da eine Webseite tausende Seiten enthalten kann, gehen wir hierbei in Batches vor. Zuerst extrahieren mehrere KI-Agenten aus jeweils 500 URLs ein Sample Set und begründen ihre Auswahlen. Es ist wichtig eine gute Begründung zu geben, damit der zusammenfassende KI-Agent die Gedanken hinter den Auswahlen besser versteht.\n",
    "    - Dann nimmt ein zweiter KI-Agent die Batches und kombiniert diese zu einem finalen Sample Set, indem er versucht die besten URLs auszuwählen. Dabei versucht er auf maximal 20 URLs zu kommen.\n",
    "    - Das Sample Set wird dann später heruntergeladen und vom Nutzer analysiert.\n",
    "</Prozess>\"\"\"\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_BATCH_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welcher die Auswahl der URLs für das Sample Set vornimmt und dabei einen Batch von maximal 500 URLs bearbeitet. Du bist also nicht der, welcher am Ende die ganzen Batches zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<URLs>\n",
    "- Hier sind die URLs, aus welchen du ein Sample Set auswählen sollst:\n",
    "{urls_text}\n",
    "</URLs>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer die ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welche die ganzen Batches zu einem finalen Sample Set zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<Batch Sample Sets>\n",
    "- Hier sind die Batches, welche du zusammenfassen sollst:\n",
    "{batch_llm_outputs_text}\n",
    "</Batch Sample Sets>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer die ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "## LLM Output models\n",
    "\n",
    "\n",
    "class InterestingUrlsSingleResultlLlmOutput(BaseModel):\n",
    "    url: str = Field()\n",
    "    reason: str = Field()\n",
    "\n",
    "\n",
    "class InterestingUrlsFullResultLlmOutput(BaseModel):\n",
    "    url_infos: list[InterestingUrlsSingleResultlLlmOutput] = Field()\n",
    "\n",
    "\n",
    "## System prompt methods\n",
    "\n",
    "\n",
    "def generate_interesting_urls_batch_system_prompt(\n",
    "    urls: list[str], custom_instructions: str\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(INTERESTING_URLS_EXTRACTION_COMMON_SYSTEM_PROMPT)\n",
    "\n",
    "    urls_text = \"\\n\".join(\n",
    "        [f\"{x_index + 1}. {x_url}\" for x_index, x_url in enumerate(urls)]\n",
    "    )\n",
    "    batch_system_prompt = INTERESTING_URLS_EXTRACTION_BATCH_SYSTEM_PROMPT.format(\n",
    "        urls_text=urls_text, custom_instructions=custom_instructions\n",
    "    )\n",
    "    parts.append(batch_system_prompt)\n",
    "\n",
    "    system_prompt = \"\\n\\n\".join(parts)\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "def generate_interesting_urls_summarizer_system_prompt(\n",
    "    batch_full_result_llm_outputs: list[InterestingUrlsFullResultLlmOutput],\n",
    "    custom_instructions: str,\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT)\n",
    "\n",
    "    batch_llm_outputs_text = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Batch {x_index + 1}:\\n\"\n",
    "            + \"\\n\".join(\n",
    "                [\n",
    "                    f\"- {y_single_result_llm_output.url}\\n  Reason: {y_single_result_llm_output.reason}\"\n",
    "                    for y_single_result_llm_output in x_batch_full_resultllm_output.url_infos\n",
    "                ]\n",
    "            )\n",
    "            for x_index, x_batch_full_resultllm_output in enumerate(\n",
    "                batch_full_result_llm_outputs\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    system_prompt = INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT.format(\n",
    "        batch_llm_outputs_text=batch_llm_outputs_text,\n",
    "        custom_instructions=custom_instructions,\n",
    "    )\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "## File helpers\n",
    "\n",
    "### Interesting URLs folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_folder_path() -> Path:\n",
    "    project_folder_path = get_project_dir_path()\n",
    "    file_path = project_folder_path / \"10_interesting_urls\"\n",
    "    file_path.mkdir(exist_ok=True, parents=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "### Results folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_results_folder_path() -> Path:\n",
    "    interesting_urls_folder_path = get_interesting_urls_folder_path()\n",
    "    results_folder_path = interesting_urls_folder_path / \"results\"\n",
    "    results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return results_folder_path\n",
    "\n",
    "\n",
    "### Interesting URLs file\n",
    "\n",
    "\n",
    "def save_interesting_urls(urls: list[str]) -> None:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    file_path = interesting_urls_results_folder_path / \"interesting_urls.txt\"\n",
    "    urls_text = \"\\n\".join(urls)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(urls_text)\n",
    "\n",
    "\n",
    "def get_interesting_urls() -> list[str]:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    interesting_urls_file_path = (\n",
    "        interesting_urls_results_folder_path / \"interesting_urls.txt\"\n",
    "    )\n",
    "    with open(interesting_urls_file_path, \"r\") as file:\n",
    "        urls = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return urls\n",
    "\n",
    "\n",
    "### Interesting URLs reason file\n",
    "\n",
    "\n",
    "def save_interesting_urls_reason(\n",
    "    llm_output: InterestingUrlsFullResultLlmOutput,\n",
    ") -> None:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    file_path = interesting_urls_results_folder_path / \"interesting_urls_reasons.txt\"\n",
    "    reason = llm_output.model_dump_json(indent=2)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(reason)\n",
    "\n",
    "\n",
    "### Interesting URLs downloads folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_downloads_folder_path() -> Path:\n",
    "    interesting_urls_folder_path = get_interesting_urls_folder_path()\n",
    "    downloaded_folder_path = interesting_urls_folder_path / \"downloads\"\n",
    "    downloaded_folder_path.mkdir(exist_ok=True)\n",
    "    return downloaded_folder_path\n",
    "\n",
    "\n",
    "### Interesting URLs downloads file\n",
    "\n",
    "\n",
    "def save_downloaded_url(index: int, url: str, html_content: str) -> None:\n",
    "    downloads_folder_path = get_interesting_urls_downloads_folder_path()\n",
    "    index_file_name_part = f\"{index:03d}\"\n",
    "    url_file_name_part = convert_url_to_file_name(url)\n",
    "    full_file_name = f\"{index_file_name_part}_{url_file_name_part}.html\"\n",
    "    file_path = downloads_folder_path / full_file_name\n",
    "    with open(file_path) as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "async def download_interesting_urls() -> None:\n",
    "    log_message = \"Downloading interesting URLs...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    urls = get_interesting_urls()\n",
    "\n",
    "    log_message = f\"Found {len(urls)} URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=60) as http_client:\n",
    "        download_tasks = [http_client.get(x_url) for x_url in urls]\n",
    "        responses = await asyncio.gather(*download_tasks)\n",
    "\n",
    "    data_packs: list[SaveStateDataPack] = []\n",
    "\n",
    "    for x_url, x_response in zip(urls, responses):\n",
    "        soup = BeautifulSoup(x_response.text, \"html.parser\")\n",
    "        prettified_html = soup.prettify()\n",
    "\n",
    "        data_pack = SaveStateDataPack(\n",
    "            url=x_url,\n",
    "            raw_html=prettified_html,\n",
    "            cleaned_html=\"\",\n",
    "            raw_markdown=\"\",\n",
    "            cleaned_markdown=\"\",\n",
    "            feedback=\"\",\n",
    "        )\n",
    "        data_packs.append(data_pack)\n",
    "\n",
    "    results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    downloads_folder_path = get_interesting_urls_downloads_folder_path()\n",
    "    save_state = SaveState(\n",
    "        versions_folder_path=downloads_folder_path,\n",
    "        cloneable_result_folder_path=results_folder_path,\n",
    "        data_packs=data_packs,\n",
    "    )\n",
    "    save_save_state(save_state)\n",
    "\n",
    "    log_message = f\"Downloaded {len(urls)} URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "\n",
    "async def extract_interesting_urls(custom_instructions: str) -> None:\n",
    "    log_message = \"Extracting interesting URLs...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    urls = get_saved_sitemap_urls()\n",
    "\n",
    "    log_message = f\"Found {len(urls)} URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    url_batches = batch_items(urls, INTERESTING_URLS_EXTRACTION_BATCH_SIZE)\n",
    "    batch_full_result_llm_outputs: list[InterestingUrlsFullResultLlmOutput] = []\n",
    "\n",
    "    batch_system_prompts = [\n",
    "        generate_interesting_urls_batch_system_prompt(x_url_batch, custom_instructions)\n",
    "        for x_url_batch in url_batches\n",
    "    ]\n",
    "    batch_full_result_llm_outputs = await call_structured_llm_batch(\n",
    "        batch_system_prompts,\n",
    "        InterestingUrlsFullResultLlmOutput,\n",
    "    )\n",
    "\n",
    "    log_message = f\"Summarizing {len(batch_full_result_llm_outputs)} batches...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    system_prompt = generate_interesting_urls_summarizer_system_prompt(\n",
    "        batch_full_result_llm_outputs, custom_instructions\n",
    "    )\n",
    "    summarized_full_result_llm_output = await call_structured_llm(\n",
    "        system_prompt,\n",
    "        InterestingUrlsFullResultLlmOutput,\n",
    "    )\n",
    "    urls = [\n",
    "        x_url_info.url for x_url_info in summarized_full_result_llm_output.url_infos\n",
    "    ]\n",
    "\n",
    "    log_message = f\"Summarized {len(batch_full_result_llm_outputs)} batches and found {len(urls)} interesting URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    save_interesting_urls(urls)\n",
    "    save_interesting_urls_reason(summarized_full_result_llm_output)\n",
    "\n",
    "    log_message = f\"Extracted {len(urls)} interesting URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    await download_interesting_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af039f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS selectors\n",
    "\n",
    "## Prompts\n",
    "\n",
    "CSS_SELECTORS_EXTRACTION_SYSTEM_PROMPT = \"\"\"<Prozess>\n",
    "- Du bist Teil des folgenden Prozesses:\n",
    "    - Um die Ausschließ-CSS-Selektoren zu bestimmen, wird ein Sample Set an Unterseiten der Webseite analysiert.\n",
    "    - Dabei werden die heruntergeladenen HTML-Seiten betrachtet und Ausschließ-CSS-Selektoren identifiziert, die \"Verschmutzungen\" entfernen (Header, Footer, Cookiebanner, Werbung, etc.).\n",
    "    - Dabei soll alles entfernt werden, was nicht wirklichen Inhalt darstellt. Natürlich sollen Titel, Beschreibungen, Kontaktdaten, Öffnungszeiten, etc. alles bleiben.\n",
    "    - Aber viel bei einer Webseite ist auch einfach um die eigentlichen Inhalte \"drumherum\", z. B. Nav, Footer, PopUps, Bedienungshilfen, Socialmedia Widgets, Breadcrumbs, etc. diese sollen alle entfernt werden.\n",
    "    - Das Ziel ist es, CSS-Selektoren zu finden, welche auf allen Seiten anwendbar sind, da am Ende das gleiche Set für alle tausende Seiten der Webseite verwendet wird.\n",
    "    - CSS-Selektoren sollten so generisch wie möglich gehalten werden und z. B. nicht auf bestimmte Titel auf bestimmten Seiten abzielen.\n",
    "    - Es sollte auch immer das höchstmögliche Element targetiert werden, z. B. sollte natürlich nicht jeder Button einzelnd in einem Cookie-Banner entfernt werden, sondern direkt der ganze Banner oder wenn dieser im Footer ist, welcher auch weg soll, dann direkt der gesamte Footer. So minimieren wir die benötigte Anzahl an CSS-Selektoren.\n",
    "    - Da mit etwa 20 Sample Seiten gearbeitet wird und das rohe HTML noch sehr lang ist, wird in Batches vorgegangen. Zuerst analysieren mehrere KI-Agenten immer 1 Seite und identifizieren perfekte CSS-Selektoren und begründen auch ihre Auswahl. Es ist wichtig eine gute Begründung zu geben, damit der zusammenfassende KI-Agent die Gedanken hinter den Auswahlen besser versteht. Zusätzlich wird zu jedem CSS-Selektor auch eine Beispiel-Zeilennummer aus dem HTML angegeben. Durch diese wird dann ein Beispiel-Code-Block aus dem HTML geschnitten und auch dem zusammenfassenden KI-Agenten gegeben, damit dieser die Entscheidungen besser nachvollziehen kann und auch beim Kombinieren noch selbst eine gute Entscheidungsgrundlage hat.\n",
    "    - Dann nimmt ein zweiter KI-Agent die Batches und kombiniert diese zu einer finalen Liste an CSS-Selektoren, indem er versucht die CSS-Selektoren so generisch wie möglich zu kombinieren und versucht sicherzustellen, dass die CSS-Selektoren auf allen Seiten anwendbar sind. Dabei gibt es keine Limitierung für die Anzahl an CSS-Selektoren.\n",
    "    - Diese Selektoren werden dann später in neuracrawl verwendet, um sauberes Markdown zu extrahieren.\n",
    "</Prozess>\"\"\"\n",
    "\n",
    "CSS_SELECTORS_EXTRACTION_BATCH_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welcher die CSS-Selektoren für eine einzelne HTML-Seite identifiziert. Du bist also nicht der, welcher am Ende die ganzen Batches zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<HTML>\n",
    "- Hier ist die HTML-Seite, welche du analysieren sollst (mit Zeilennummern):\n",
    "{html_text}\n",
    "</HTML>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer dir ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "CSS_SELECTORS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welcher die ganzen Batches zu einer finalen Liste an CSS-Selektoren zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<Batch CSS Selectors>\n",
    "- Hier sind die Batches, welche du zusammenfassen sollst:\n",
    "{batch_llm_outputs_text}\n",
    "</Batch CSS Selectors>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer dir ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "## LLM Output models\n",
    "\n",
    "\n",
    "class CssSelectorSingleResultLlmOutput(BaseModel):\n",
    "    css_selector: str = Field()\n",
    "    reason: str = Field()\n",
    "    example_line_number: int = Field()\n",
    "\n",
    "\n",
    "class CssSelectorsFullResultLlmOutput(BaseModel):\n",
    "    css_selector_infos: list[CssSelectorSingleResultLlmOutput] = Field()\n",
    "\n",
    "\n",
    "## Models\n",
    "\n",
    "\n",
    "class ExtendedCssSelectorSingleResult(CssSelectorSingleResultLlmOutput):\n",
    "    example_html: str = Field()\n",
    "\n",
    "\n",
    "class ExtendedCssSelectorsFullResult(BaseModel):\n",
    "    url: str = Field()\n",
    "    css_selector_infos: list[ExtendedCssSelectorSingleResult] = Field()\n",
    "\n",
    "\n",
    "## System prompt methods\n",
    "\n",
    "\n",
    "def generate_css_selectors_batch_system_prompt(\n",
    "    html_content: str, custom_instructions: str\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(CSS_SELECTORS_EXTRACTION_SYSTEM_PROMPT)\n",
    "\n",
    "    # Add line numbers to HTML\n",
    "    html_lines = html_content.split(\"\\n\")\n",
    "    numbered_html_lines = [\n",
    "        f\"{x_index + 1:5d} | {x_line}\" for x_index, x_line in enumerate(html_lines)\n",
    "    ]\n",
    "    html_text = \"\\n\".join(numbered_html_lines)\n",
    "    batch_system_prompt = CSS_SELECTORS_EXTRACTION_BATCH_SYSTEM_PROMPT.format(\n",
    "        html_text=html_text, custom_instructions=custom_instructions\n",
    "    )\n",
    "    parts.append(batch_system_prompt)\n",
    "\n",
    "    system_prompt = \"\\n\\n\".join(parts)\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "def generate_css_selectors_summarizer_system_prompt(\n",
    "    batch_llm_outputs: list[ExtendedCssSelectorsFullResult],\n",
    "    custom_instructions: str,\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(CSS_SELECTORS_EXTRACTION_SYSTEM_PROMPT)\n",
    "\n",
    "    batch_llm_outputs_text = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Batch {x_index + 1} (URL: {x_batch_llm_output.url}):\\n\"\n",
    "            + \"\\n\".join(\n",
    "                [\n",
    "                    f\"- CSS Selector: {y_selector_output.css_selector}\\n  Reason: {y_selector_output.reason}\\n  Example HTML:\\n{y_selector_output.example_html}\"\n",
    "                    for y_selector_output in x_batch_llm_output.css_selector_infos\n",
    "                ]\n",
    "            )\n",
    "            for x_index, x_batch_llm_output in enumerate(batch_llm_outputs)\n",
    "        ]\n",
    "    )\n",
    "    system_prompt = CSS_SELECTORS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT.format(\n",
    "        batch_llm_outputs_text=batch_llm_outputs_text,\n",
    "        custom_instructions=custom_instructions,\n",
    "    )\n",
    "    parts.append(system_prompt)\n",
    "\n",
    "    system_prompt = \"\\n\\n\".join(parts)\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "## File helpers\n",
    "\n",
    "### CSS selectors folder\n",
    "\n",
    "\n",
    "def get_css_selectors_folder_path() -> Path:\n",
    "    project_folder_path = get_project_dir_path()\n",
    "    file_path = project_folder_path / \"20_css_selectors\"\n",
    "    file_path.mkdir(exist_ok=True, parents=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "### Results folder\n",
    "\n",
    "\n",
    "def get_css_selectors_results_folder_path() -> Path:\n",
    "    css_selectors_folder_path = get_css_selectors_folder_path()\n",
    "    results_folder_path = css_selectors_folder_path / \"results\"\n",
    "    results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return results_folder_path\n",
    "\n",
    "\n",
    "### CSS selectors file\n",
    "\n",
    "\n",
    "def save_css_selectors(css_selectors: list[str]) -> None:\n",
    "    css_selectors_results_folder_path = get_css_selectors_results_folder_path()\n",
    "    file_path = css_selectors_results_folder_path / \"css_selectors.txt\"\n",
    "    selectors_text = \"\\n\".join(css_selectors)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(selectors_text)\n",
    "\n",
    "\n",
    "### CSS selectors reason file\n",
    "\n",
    "\n",
    "def save_css_selectors_reason(\n",
    "    llm_output: CssSelectorsFullResultLlmOutput,\n",
    ") -> None:\n",
    "    css_selectors_results_folder_path = get_css_selectors_results_folder_path()\n",
    "    file_path = css_selectors_results_folder_path / \"css_selectors_reasons.txt\"\n",
    "    reason = llm_output.model_dump_json(indent=2)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(reason)\n",
    "\n",
    "\n",
    "## Utilities\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def extract_example_html_lines(html_content: str, line_number: int) -> str:\n",
    "    html_lines = html_content.split(\"\\n\")\n",
    "    start_index = max(\n",
    "        0, line_number - 4\n",
    "    )  # -4 because line_number is 1-indexed and we want 3 lines before\n",
    "    end_index = min(len(html_lines), line_number + 3)  # +3 to get 3 lines after\n",
    "\n",
    "    example_lines = html_lines[start_index:end_index]\n",
    "    numbered_example_lines = [\n",
    "        f\"{start_index + x_index + 1:5d} | {x_line}\"\n",
    "        for x_index, x_line in enumerate(example_lines)\n",
    "    ]\n",
    "    return \"\\n\".join(numbered_example_lines)\n",
    "\n",
    "\n",
    "def hydrate_full_results_to_extended_full_results(\n",
    "    full_results: list[CssSelectorsFullResultLlmOutput],\n",
    "    data_packs: list[SaveStateDataPack],\n",
    ") -> list[ExtendedCssSelectorsFullResult]:\n",
    "    extended_full_results: list[ExtendedCssSelectorsFullResult] = []\n",
    "\n",
    "    for x_data_pack, x_llm_output in zip(data_packs, full_results):\n",
    "        extended_single_results: list[ExtendedCssSelectorSingleResult] = []\n",
    "\n",
    "        for x_css_selector_info in x_llm_output.css_selector_infos:\n",
    "            example_html = extract_example_html_lines(\n",
    "                x_data_pack.raw_html, x_css_selector_info.example_line_number\n",
    "            )\n",
    "            extended_single_result = ExtendedCssSelectorSingleResult(\n",
    "                css_selector=x_css_selector_info.css_selector,\n",
    "                reason=x_css_selector_info.reason,\n",
    "                example_line_number=x_css_selector_info.example_line_number,\n",
    "                example_html=example_html,\n",
    "            )\n",
    "            extended_single_results.append(extended_single_result)\n",
    "\n",
    "        extended_full_result = ExtendedCssSelectorsFullResult(\n",
    "            url=x_data_pack.url,\n",
    "            css_selector_infos=extended_single_results,\n",
    "        )\n",
    "        extended_full_results.append(extended_full_result)\n",
    "\n",
    "    return extended_full_results\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "async def extract_css_selectors(\n",
    "    custom_instructions: str, interesting_urls_save_state_version: str | None = None\n",
    ") -> None:\n",
    "    log_message = \"Extracting CSS selectors...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    interesting_urls_downloads_folder_path = (\n",
    "        get_interesting_urls_downloads_folder_path()\n",
    "    )\n",
    "    interesting_urls_save_state = get_save_state(\n",
    "        interesting_urls_downloads_folder_path, interesting_urls_save_state_version\n",
    "    )\n",
    "    data_packs = interesting_urls_save_state.data_packs\n",
    "\n",
    "    log_message = f\"Found {len(data_packs)} pages.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    batch_system_prompts = [\n",
    "        generate_css_selectors_batch_system_prompt(\n",
    "            x_data_pack.raw_html, custom_instructions\n",
    "        )\n",
    "        for x_data_pack in data_packs\n",
    "    ]\n",
    "    for x_prompt in batch_system_prompts:\n",
    "        print(len(x_prompt))\n",
    "    batch_full_results = await call_structured_llm_batch(\n",
    "        batch_system_prompts,\n",
    "        CssSelectorsFullResultLlmOutput,\n",
    "    )\n",
    "    extended_batch_full_results = hydrate_full_results_to_extended_full_results(\n",
    "        batch_full_results, data_packs\n",
    "    )\n",
    "\n",
    "    log_message = f\"Summarizing {len(extended_batch_full_results)} batches...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    system_prompt = generate_css_selectors_summarizer_system_prompt(\n",
    "        extended_batch_full_results, custom_instructions\n",
    "    )\n",
    "    summarized_full_result = await call_structured_llm(\n",
    "        system_prompt,\n",
    "        CssSelectorsFullResultLlmOutput,\n",
    "    )\n",
    "    css_selectors = [\n",
    "        x_selector_output.css_selector\n",
    "        for x_selector_output in summarized_full_result.css_selector_infos\n",
    "    ]\n",
    "\n",
    "    log_message = f\"Summarized {len(extended_batch_full_results)} batches and found {len(css_selectors)} CSS selectors.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    save_css_selectors(css_selectors)\n",
    "    save_css_selectors_reason(summarized_full_result)\n",
    "\n",
    "    log_message = f\"Extracted {len(css_selectors)} CSS selectors.\"\n",
    "    logger.info(log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "547231af",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Setup done.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[149]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSetup done.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Setup done."
     ]
    }
   ],
   "source": [
    "raise Exception(\"Setup done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd989b",
   "metadata": {},
   "source": [
    "# neuracrawl Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "526791d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your project name here\n",
    "\n",
    "set_project(\"siegburg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c957d",
   "metadata": {},
   "source": [
    "## Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "647e69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sitemap_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "13b3fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168 common URL areas with frequency >= 5\n"
     ]
    }
   ],
   "source": [
    "extract_frequent_sitemap_urls(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe84d31",
   "metadata": {},
   "source": [
    "# Exclusion URL Regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "await extract_url_regexes(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af9296",
   "metadata": {},
   "source": [
    "## Interesting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df22df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracting interesting URLs...\n",
      "INFO:__main__:Found 6883 URLs.\n",
      "INFO:__main__:Processing prompt (1/14)...\n",
      "INFO:__main__:Processing prompt (2/14)...\n",
      "INFO:__main__:Processing prompt (3/14)...\n",
      "INFO:__main__:Processing prompt (4/14)...\n",
      "INFO:__main__:Processing prompt (5/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (4/14).\n",
      "INFO:__main__:Processing prompt (6/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (5/14).\n",
      "INFO:__main__:Processing prompt (7/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (1/14).\n",
      "INFO:__main__:Processing prompt (8/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (2/14).\n",
      "INFO:__main__:Processing prompt (9/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (3/14).\n",
      "INFO:__main__:Processing prompt (10/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (6/14).\n",
      "INFO:__main__:Processing prompt (11/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (9/14).\n",
      "INFO:__main__:Processing prompt (12/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (11/14).\n",
      "INFO:__main__:Processing prompt (13/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (7/14).\n",
      "INFO:__main__:Processing prompt (14/14)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (8/14).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (10/14).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (12/14).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (14/14).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (13/14).\n",
      "INFO:__main__:Summarizing 14 batches...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Summarized 14 batches and found 30 interesting URLs.\n",
      "INFO:__main__:Extracted 30 interesting URLs.\n",
      "INFO:__main__:Downloading interesting URLs...\n",
      "INFO:__main__:Found 30 URLs.\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/newsletter/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/wirtschaft-handel/ \"HTTP/1.1 301 301\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/impressum/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/nachrichten/2024/dezember/haushalt-verabschiedet/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/orte/sehenswuerdigkeiten/abtei-michaelsberg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/service-verwaltung/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/datenschutz/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/kindertagesstaetten/katholische-kindertagesstaette-liebfrauen/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/familie-bildung-soziales/kinderbetreuung/kindertagesstaetten/die-deichmaeuse/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/serviceportal-a-z/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/cityportal/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/nachrichten/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/abteilungen/NRW:department:15529/stadtverwaltung-siegburg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/cityportal/gastronomiebetriebe/anno17/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/stadtportraet/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/familie-bildung-soziales/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/planen-bauen-verkehr/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/cityportal/firmen-unternehmen/absolut-designhaus/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/feuerwehr-siegburg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/aktuelles/newsletter/archiv/2024/dezember/siegburgaktuell-15-12-2024/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/leistungen/NRW:entry:42373-VLR/terminreservierung-online/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/mitarbeiter/NRW:employee:62386/glathe-carsten/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/schulen/anno-gymnasium-siegburg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/leistungen/NRW:entry:42049-VLR/personalausweis/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/sitemap/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/umwelt-klimaschutz/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/externe-datenquellen/siegburg-veranstaltungen/2226:0/herman-van-veen/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/kalender/kombinierter-kalender/ \"HTTP/1.1 200 200\"\n",
      "INFO:__main__:Downloaded 30 URLs.\n"
     ]
    }
   ],
   "source": [
    "await extract_interesting_urls(\"\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1fc1b",
   "metadata": {},
   "source": [
    "# CSS Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "11406201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracting CSS selectors...\n",
      "INFO:__main__:Found 30 pages.\n",
      "644374\n",
      "441974\n",
      "452629\n",
      "440628\n",
      "438164\n",
      "496363\n",
      "441218\n",
      "469797\n",
      "466240\n",
      "427490\n",
      "477674\n",
      "479044\n",
      "474692\n",
      "457612\n",
      "446291\n",
      "450795\n",
      "557665\n",
      "533414\n",
      "457424\n",
      "458004\n",
      "451562\n",
      "442750\n",
      "449574\n",
      "444736\n",
      "947116\n",
      "5090\n",
      "448890\n",
      "525635\n",
      "450429\n",
      "467144\n",
      "INFO:__main__:Processing prompt (1/30)...\n",
      "INFO:__main__:Processing prompt (2/30)...\n",
      "INFO:__main__:Processing prompt (3/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (2/30).\n",
      "INFO:__main__:Processing prompt (4/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (1/30).\n",
      "INFO:__main__:Processing prompt (5/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (3/30).\n",
      "INFO:__main__:Processing prompt (6/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (4/30).\n",
      "INFO:__main__:Processing prompt (7/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (5/30).\n",
      "INFO:__main__:Processing prompt (8/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (6/30).\n",
      "INFO:__main__:Processing prompt (9/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (7/30).\n",
      "INFO:__main__:Processing prompt (10/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (8/30).\n",
      "INFO:__main__:Processing prompt (11/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (9/30).\n",
      "INFO:__main__:Processing prompt (12/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (10/30).\n",
      "INFO:__main__:Processing prompt (13/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (11/30).\n",
      "INFO:__main__:Processing prompt (14/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (12/30).\n",
      "INFO:__main__:Processing prompt (15/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (13/30).\n",
      "INFO:__main__:Processing prompt (16/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (14/30).\n",
      "INFO:__main__:Processing prompt (17/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (15/30).\n",
      "INFO:__main__:Processing prompt (18/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (16/30).\n",
      "INFO:__main__:Processing prompt (19/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (17/30).\n",
      "INFO:__main__:Processing prompt (20/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (19/30).\n",
      "INFO:__main__:Processing prompt (21/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (20/30).\n",
      "INFO:__main__:Processing prompt (22/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (18/30).\n",
      "INFO:__main__:Processing prompt (23/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (21/30).\n",
      "INFO:__main__:Processing prompt (24/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (22/30).\n",
      "INFO:__main__:Processing prompt (25/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (23/30).\n",
      "INFO:__main__:Processing prompt (26/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (26/30).\n",
      "INFO:__main__:Processing prompt (27/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (24/30).\n",
      "INFO:__main__:Processing prompt (28/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (25/30).\n",
      "INFO:__main__:Processing prompt (29/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (27/30).\n",
      "INFO:__main__:Processing prompt (30/30)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (28/30).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (29/30).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (30/30).\n",
      "INFO:__main__:Summarizing 30 batches...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Summarized 30 batches and found 18 CSS selectors.\n",
      "INFO:__main__:Extracted 18 CSS selectors.\n"
     ]
    }
   ],
   "source": [
    "await extract_css_selectors(\"\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844086a",
   "metadata": {},
   "source": [
    "[] need funcs to create new project\n",
    "[] viewer should use save_state.json, maybe only work with save_state.json instead of all the other files? then have a result object that gets plugged into the save_state.json that can include extra data?\n",
    "- pre cleaning of images, scripts, links, styles etc\n",
    "- then insta clean with css selectors and markdown conversion and markdown cleaning, all in here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
