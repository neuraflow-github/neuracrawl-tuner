{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1fd78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from collections import Counter\n",
    "from logging import getLogger\n",
    "from pathlib import Path\n",
    "\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from crawl4ai.html2text import CustomHTML2Text\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Logging\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "# Utilities\n",
    "\n",
    "\n",
    "def convert_url_to_file_name(url: str) -> str:\n",
    "    filename = url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "def batch_items[T](\n",
    "    items: list[T],\n",
    "    max_item_count_per_batch: int,\n",
    ") -> list[list[T]]:\n",
    "    batches = [\n",
    "        items[x_index : x_index + max_item_count_per_batch]\n",
    "        for x_index in range(0, len(items), max_item_count_per_batch)\n",
    "    ]\n",
    "    return batches\n",
    "\n",
    "\n",
    "## LLMs\n",
    "\n",
    "\n",
    "async def call_structured_llm[T: BaseModel](\n",
    "    system_prompt: str,\n",
    "    output_model: type[T],\n",
    ") -> T:\n",
    "    system_message = SystemMessage(system_prompt)\n",
    "    human_message = HumanMessage(\"Erledige die Aufgabe.\")\n",
    "    messages = [system_message, human_message]\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=\"http://127.0.0.1:50025\",\n",
    "        api_key=\"litellm-api-key-1234\",\n",
    "        # model=\"bedrock/eu.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        model=\"gemini/gemini-2.5-pro\",\n",
    "        timeout=120,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    structured_llm = llm.with_structured_output(\n",
    "        output_model,\n",
    "        method=\"json_schema\",\n",
    "    )\n",
    "    llm_output = await structured_llm.ainvoke(messages)\n",
    "    return llm_output\n",
    "\n",
    "\n",
    "MAX_CONCURRENT_BATCH_COUNT = 20\n",
    "\n",
    "\n",
    "async def call_structured_llm_batch[T: BaseModel](\n",
    "    system_prompts: list[str],\n",
    "    output_model: type[T],\n",
    ") -> list[T]:\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_BATCH_COUNT)\n",
    "\n",
    "    async def process_rate_limited(\n",
    "        batch_index: int,\n",
    "        system_prompt: str,\n",
    "    ) -> T:\n",
    "        async with semaphore:\n",
    "            log_message = (\n",
    "                f\"Processing prompt ({batch_index + 1}/{len(system_prompts)})...\"\n",
    "            )\n",
    "            logger.info(log_message)\n",
    "\n",
    "            result = await call_structured_llm(system_prompt, output_model)\n",
    "\n",
    "            log_message = f\"Processed prompt ({batch_index + 1}/{len(system_prompts)}).\"\n",
    "            logger.info(log_message)\n",
    "\n",
    "            return result\n",
    "\n",
    "    process_tasks = [\n",
    "        process_rate_limited(x_index, x_system_prompt)\n",
    "        for x_index, x_system_prompt in enumerate(system_prompts)\n",
    "    ]\n",
    "\n",
    "    all_results = await asyncio.gather(*process_tasks)\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Sub state\n",
    "\n",
    "\n",
    "class SubStateDataPack(BaseModel):\n",
    "    url: str = Field()\n",
    "    raw_html: str = Field()\n",
    "    cleaned_html: str = Field()\n",
    "    raw_markdown: str = Field()\n",
    "    cleaned_markdown: str = Field()\n",
    "    feedback: str = Field()\n",
    "\n",
    "\n",
    "class SubState(BaseModel):\n",
    "    versions_folder_path: Path = Field()\n",
    "    cloneable_result_folder_path: Path = Field()\n",
    "    sub_state_data_packs: list[SubStateDataPack] = Field()\n",
    "\n",
    "\n",
    "class SubStateManager:\n",
    "    def get_current_versions(self, folder_path: Path) -> list[str]:\n",
    "        versions: list[str] = []\n",
    "\n",
    "        for x_folder_path in folder_path.iterdir():\n",
    "            if x_folder_path.is_dir() and x_folder_path.name.startswith(\"v_\"):\n",
    "                versions.append(x_folder_path.name)\n",
    "\n",
    "        sorted_versions = sorted(versions)\n",
    "        return sorted_versions\n",
    "\n",
    "    def get_latest_version(self, folder_path: Path) -> str:\n",
    "        versions = self.get_current_versions(folder_path)\n",
    "        if len(versions) == 0:\n",
    "            return \"v_000\"\n",
    "        latest_version = versions[-1]\n",
    "        return latest_version\n",
    "\n",
    "    def get_next_version(self, folder_path: Path) -> str:\n",
    "        latest_version = self.get_latest_version(folder_path)\n",
    "        if latest_version == \"v_000\" and not (folder_path / latest_version).exists():\n",
    "            return \"v_000\"\n",
    "\n",
    "        version_number = int(latest_version.split(\"_\")[1])\n",
    "        next_version_number = version_number + 1\n",
    "        next_version = f\"v_{next_version_number:03d}\"\n",
    "        return next_version\n",
    "\n",
    "    def save_sub_state(self, sub_state: SubState) -> None:\n",
    "        next_version = self.get_next_version(sub_state.versions_folder_path)\n",
    "        version_folder_path = sub_state.versions_folder_path / next_version\n",
    "        version_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        sub_state_file_path = version_folder_path / \"save_state.json\"\n",
    "        sub_state_json = sub_state.model_dump_json(indent=2)\n",
    "        sub_state_file_path.write_text(sub_state_json)\n",
    "\n",
    "        results_folder_path = version_folder_path / \"results\"\n",
    "        results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        if sub_state.cloneable_result_folder_path.exists():\n",
    "            for x_item in sub_state.cloneable_result_folder_path.iterdir():\n",
    "                if x_item.is_file():\n",
    "                    shutil.copy2(x_item, results_folder_path / x_item.name)\n",
    "                elif x_item.is_dir():\n",
    "                    shutil.copytree(\n",
    "                        x_item, results_folder_path / x_item.name, dirs_exist_ok=True\n",
    "                    )\n",
    "\n",
    "        for x_index, x_sub_state_data_pack in enumerate(sub_state.sub_state_data_packs):\n",
    "            sub_state_data_pack_folder_name = (\n",
    "                f\"{x_index:03d}_{convert_url_to_file_name(x_sub_state_data_pack.url)}\"\n",
    "            )\n",
    "            sub_state_data_pack_folder_path = (\n",
    "                version_folder_path / sub_state_data_pack_folder_name\n",
    "            )\n",
    "            sub_state_data_pack_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            url_file_path = sub_state_data_pack_folder_path / \"05_url.txt\"\n",
    "            url_file_path.write_text(x_sub_state_data_pack.url)\n",
    "\n",
    "            raw_html_file_path = sub_state_data_pack_folder_path / \"10_raw_html.html\"\n",
    "            raw_html_file_path.write_text(x_sub_state_data_pack.raw_html)\n",
    "\n",
    "            cleaned_html_file_path = (\n",
    "                sub_state_data_pack_folder_path / \"20_cleaned_html.html\"\n",
    "            )\n",
    "            cleaned_html_file_path.write_text(x_sub_state_data_pack.cleaned_html)\n",
    "\n",
    "            raw_markdown_file_path = (\n",
    "                sub_state_data_pack_folder_path / \"30_raw_markdown.md\"\n",
    "            )\n",
    "            raw_markdown_file_path.write_text(x_sub_state_data_pack.raw_markdown)\n",
    "\n",
    "            cleaned_markdown_file_path = (\n",
    "                sub_state_data_pack_folder_path / \"40_cleaned_markdown.md\"\n",
    "            )\n",
    "            cleaned_markdown_file_path.write_text(\n",
    "                x_sub_state_data_pack.cleaned_markdown\n",
    "            )\n",
    "\n",
    "            feedback_file_path = sub_state_data_pack_folder_path / \"50_feedback.txt\"\n",
    "            feedback_file_path.write_text(x_sub_state_data_pack.feedback)\n",
    "\n",
    "    def get_sub_state(\n",
    "        self, versions_folder_path: Path, version: str | None = None\n",
    "    ) -> SubState:\n",
    "        if version is None:\n",
    "            version_folders = sorted(\n",
    "                [\n",
    "                    x_folder\n",
    "                    for x_folder in versions_folder_path.iterdir()\n",
    "                    if x_folder.is_dir() and x_folder.name.startswith(\"v\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if not version_folders:\n",
    "                raise ValueError(f\"No version folders found in {versions_folder_path}\")\n",
    "\n",
    "            version_folder_path = version_folders[-1]\n",
    "        else:\n",
    "            version_folder_path = versions_folder_path / version\n",
    "\n",
    "            if not version_folder_path.exists():\n",
    "                raise ValueError(f\"Version folder {version_folder_path} does not exist\")\n",
    "\n",
    "        sub_state_file_path = version_folder_path / \"save_state.json\"\n",
    "\n",
    "        if not sub_state_file_path.exists():\n",
    "            raise ValueError(f\"Sub state file not found at {sub_state_file_path}\")\n",
    "\n",
    "        sub_state_json = sub_state_file_path.read_text(encoding=\"utf-8\")\n",
    "        sub_state = SubState.model_validate_json(sub_state_json)\n",
    "\n",
    "        return sub_state\n",
    "\n",
    "\n",
    "SUB_STATE_MANAGER = SubStateManager()\n",
    "\n",
    "\n",
    "# Cleaner\n",
    "\n",
    "\n",
    "class Cleaner:\n",
    "    def preclean_html(self, html: str) -> str:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        preclean_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\"]\n",
    "        for x_tag in preclean_tags:\n",
    "            for x_element in soup.find_all(x_tag):\n",
    "                x_element.decompose()\n",
    "        precleaned_html = soup.prettify()\n",
    "        return precleaned_html\n",
    "\n",
    "    def clean_html(self, html: str, exclusion_css_selectors: list[str]) -> str:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for x_selector in exclusion_css_selectors:\n",
    "            for x_element in soup.select(x_selector):\n",
    "                x_element.decompose()\n",
    "        cleaned_html = soup.prettify()\n",
    "        return cleaned_html\n",
    "\n",
    "    def convert_html_to_markdown(self, html: str) -> str:\n",
    "        html_to_text_converter = CustomHTML2Text()\n",
    "        html_to_text_converter.body_width = 0\n",
    "        html_to_text_converter.ignore_links = False\n",
    "        markdown = html_to_text_converter.handle(html)\n",
    "        return markdown\n",
    "\n",
    "    def clean_markdown(self, markdown: str) -> str:\n",
    "        lines = markdown.split(\"\\n\")\n",
    "        cleaned_lines: list[str] = []\n",
    "        for x_line in lines:\n",
    "            stripped_line = x_line.strip()\n",
    "            if len(stripped_line) > 0:\n",
    "                cleaned_lines.append(x_line)\n",
    "        cleaned_markdown = \"\\n\".join(cleaned_lines)\n",
    "        return cleaned_markdown\n",
    "\n",
    "\n",
    "CLEANER = Cleaner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d5d3ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project\n",
    "\n",
    "\n",
    "class ProjectManager:\n",
    "    selected_project: str | None = None\n",
    "\n",
    "    def get_project_folder_path(self) -> Path:\n",
    "        if self.selected_project is None:\n",
    "            raise ValueError(\"No project selected.\")\n",
    "        project_folder_path = Path(os.getcwd(), \"projects\", self.selected_project)\n",
    "        return project_folder_path\n",
    "\n",
    "    def set_project(self, project_name: str) -> None:\n",
    "        self.selected_project = project_name\n",
    "        project_folder_path = self.get_project_folder_path()\n",
    "        project_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def get_project(self) -> str:\n",
    "        if self.selected_project is None:\n",
    "            raise ValueError(\"No project selected.\")\n",
    "        return self.selected_project\n",
    "\n",
    "\n",
    "PROJECT_MANAGER = ProjectManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "899a42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sitemap\n",
    "\n",
    "## File helpers\n",
    "\n",
    "### Sitemap URLS folder\n",
    "\n",
    "\n",
    "def get_sitemap_urls_folder_path() -> Path:\n",
    "    project_folder_path = PROJECT_MANAGER.get_project_folder_path()\n",
    "    folder_path = project_folder_path / \"05_sitemap_urls\"\n",
    "    folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return folder_path\n",
    "\n",
    "\n",
    "### Sitemap URLs file\n",
    "\n",
    "\n",
    "def get_sitemap_urls_txt_file_path() -> Path:\n",
    "    sitemap_urls_folder_path = get_sitemap_urls_folder_path()\n",
    "    file_path = sitemap_urls_folder_path / \"sitemap_urls.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def create_sitemap_urls_file() -> None:\n",
    "    file_path = get_sitemap_urls_txt_file_path()\n",
    "    if not file_path.exists():\n",
    "        file_path.touch()\n",
    "\n",
    "\n",
    "def save_sitemap_urls(urls: list[str]) -> None:\n",
    "    file_path = get_sitemap_urls_txt_file_path()\n",
    "    urls_text = \"\\n\".join(urls)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(urls_text)\n",
    "\n",
    "\n",
    "def get_saved_sitemap_urls() -> list[str]:\n",
    "    file_path = get_sitemap_urls_txt_file_path()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        urls = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return urls\n",
    "\n",
    "\n",
    "### Frequent Sitemap URLs file\n",
    "\n",
    "\n",
    "def get_frequent_sitemap_urls_txt_file_path() -> Path:\n",
    "    sitemap_urls_folder_path = get_sitemap_urls_folder_path()\n",
    "    file_path = sitemap_urls_folder_path / \"frequent_sitemap_urls.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_frequent_sitemap_urls(frequent_urls_text: str) -> None:\n",
    "    frequent_urls_txt_file_path = get_frequent_sitemap_urls_txt_file_path()\n",
    "    with open(frequent_urls_txt_file_path, \"w\") as file:\n",
    "        file.write(frequent_urls_text)\n",
    "\n",
    "\n",
    "def get_saved_frequent_sitemap_urls() -> str:\n",
    "    file_path = get_frequent_sitemap_urls_txt_file_path()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        frequent_urls_text = file.read()\n",
    "    return frequent_urls_text\n",
    "\n",
    "\n",
    "### Sitemap URL extensions file\n",
    "\n",
    "\n",
    "def get_sitemap_url_extensions_txt_file_path() -> Path:\n",
    "    sitemap_urls_folder_path = get_sitemap_urls_folder_path()\n",
    "    file_path = sitemap_urls_folder_path / \"sitemap_url_extensions.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_sitemap_url_extensions(url_extensions_text: str) -> None:\n",
    "    file_path = get_sitemap_url_extensions_txt_file_path()\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(url_extensions_text)\n",
    "\n",
    "\n",
    "def get_saved_sitemap_url_extensions() -> str:\n",
    "    file_path = get_sitemap_url_extensions_txt_file_path()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        url_extensions_text = file.read()\n",
    "    return url_extensions_text\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "def extract_sitemap_urls() -> None:\n",
    "    urls_txt_file_path = get_sitemap_urls_txt_file_path()\n",
    "    url_regex = r\"https?://[^\\s<>\\\"']+\"\n",
    "    with open(urls_txt_file_path, \"r\") as urls_txt_file:\n",
    "        text = urls_txt_file.read()\n",
    "    urls = re.findall(url_regex, text)\n",
    "    save_sitemap_urls(urls)\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def extract_frequent_sitemap_urls(min_frequency) -> None:\n",
    "    \"\"\"This function extracts common URL paths or path segments that appear frequently across URLs.\"\"\"\n",
    "    urls = get_saved_sitemap_urls()\n",
    "\n",
    "    # Extract path segments from each URL\n",
    "    path_segments = []\n",
    "\n",
    "    for x_url in urls:\n",
    "        # Remove protocol and domain\n",
    "        if \"://\" in x_url:\n",
    "            url_without_protocol = x_url.split(\"://\", 1)[1]\n",
    "            if \"/\" in url_without_protocol:\n",
    "                domain_and_path = url_without_protocol.split(\"/\", 1)\n",
    "                if len(domain_and_path) > 1:\n",
    "                    path = domain_and_path[1]\n",
    "                    # Build up all parent paths\n",
    "                    parts = path.rstrip(\"/\").split(\"/\")\n",
    "                    for i in range(1, len(parts) + 1):\n",
    "                        segment = \"/\".join(parts[:i]) + \"/\"\n",
    "                        path_segments.append(segment)\n",
    "\n",
    "    # Count frequency of each path segment\n",
    "    segment_counter = Counter(path_segments)\n",
    "\n",
    "    # Filter segments by minimum frequency\n",
    "    common_segments = {\n",
    "        segment: count\n",
    "        for segment, count in segment_counter.items()\n",
    "        if count >= min_frequency\n",
    "    }\n",
    "\n",
    "    # Sort by frequency (descending) and then alphabetically\n",
    "    sorted_segments = sorted(common_segments.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Write to output file\n",
    "    frequent_sitemap_urls_text = \"\\n\".join(\n",
    "        [f\"{x_count}\\t{x_segment}\" for x_segment, x_count in sorted_segments]\n",
    "    )\n",
    "    save_frequent_sitemap_urls(frequent_sitemap_urls_text)\n",
    "\n",
    "    print(\n",
    "        f\"Found {len(sorted_segments)} common URL areas with frequency >= {min_frequency}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def extract_url_extensions() -> None:\n",
    "    urls = get_saved_sitemap_urls()\n",
    "\n",
    "    url_extensions: list[str] = []\n",
    "\n",
    "    for x_url in urls:\n",
    "        if \".\" in x_url:\n",
    "            url_parts = x_url.split(\"/\")\n",
    "            last_part = url_parts[-1]\n",
    "            if \".\" in last_part:\n",
    "                extension_parts = last_part.split(\".\")\n",
    "                extension = extension_parts[-1]\n",
    "                if \"?\" in extension:\n",
    "                    extension = extension.split(\"?\")[0]\n",
    "                if \"#\" in extension:\n",
    "                    extension = extension.split(\"#\")[0]\n",
    "                if len(extension) > 0 and len(extension) <= 10:\n",
    "                    url_extensions.append(extension)\n",
    "            else:\n",
    "                url_extensions.append(\"html\")\n",
    "        else:\n",
    "            url_extensions.append(\"html\")\n",
    "\n",
    "    counter = Counter(url_extensions)\n",
    "    sorted_url_extensions = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    url_extensions_text = \"\\n\".join(\n",
    "        [\n",
    "            f\"{x_count}\\t{x_url_extension}\"\n",
    "            for x_url_extension, x_count in sorted_url_extensions\n",
    "        ]\n",
    "    )\n",
    "    save_sitemap_url_extensions(url_extensions_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fbee2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERAL_SYSTEM_PROMPT = \"\"\"<Generell>\n",
    "- neuracrawl\n",
    "    - neuracrawl ist ein Webcrawler, welcher eine Ausgangsdomain bekommt und von dort dann aus deepcrawlt, also sich durch alle Links der Seite hangelt und immer weiter nach neuen Links sucht.\n",
    "    - Er ist sehr gut darin, eine einzige Webseite sehr ausführlich zu crawlen.\n",
    "    - URL Ausschließungen\n",
    "        - Dabei schließt neuracrawl aber auch bestimmte URL Gruppen/Subpfade aus.\n",
    "        - Zum Beispiel, kann es sein, dass wir bei einer Webseite alle Veranstaltungen oder Newsartikel ausschließen wollen, da wir die zum Beispiel nochmal getrennt über eine API strukturiert auslesen.\n",
    "    - Markdown Extraktion\n",
    "        - Dabei extrahiert er extrem sauberes Markdown, ohne Header, Footer, Cookiebanner, Werbeinhalten, etc.\n",
    "        - Die Daten am Ende enthalten nur den reinen Inhalt der Webseite.\n",
    "        - Dabei geht er subtraktiv vor, also entfernt alle Elemente, welche \"Verschmutzungen\" darstellen.\n",
    "        - Dies ist immer ein Spiel zwischen \"wir wollen alles entfernen, was nicht wirklicher Inhalt ist\" und \"wir wollen nichts entfernen, was zum wirklichen Inhalt gehört\".\n",
    "        - Unser Grundsatz ist, dass wir so nah wie möglich an den wirklichen Inhalt rankommen wollen, ohne dabei aber Informationen zu verlieren. Wir dürfen auf keinen Fall echte Informationen verlieren, egal, wo diese auf der Webseite stehen.\n",
    "    - neuracrawl benötigt generell die folgenden Einstellungen:\n",
    "        - Ausgangsdomain und erlaubt andere Domains, auf welche er kommen und crawlen darf.\n",
    "        - Ausschließ-URL-Regexes, welche bestimmte URL Gruppen/Subpfade ausschließen.\n",
    "            - Zum Beispiel : \"^.*/(aktuelles|amtsblatt)/.*$\" oder \"^.*\\\\.(?:ics|pdf).*$\"\n",
    "        - Ausschließ-CSS-Selektoren, welche bestimmte HTML-Elemente auf allen Seiten ausschließen.\n",
    "            - Zum Beispiel : \"header\", \".front-left\" oder \"#cc-size\"\n",
    "\n",
    "- neuracrawl tuner ist eine Sammlung an Funktionen, welche dabei helfen, die perfekten Werte für die obigen Einstellungen zu finden.\n",
    "</Generell>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "212ee011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Regexes\n",
    "\n",
    "## Prompts\n",
    "\n",
    "URL_REGEXES_EXTRACTION_SYSTEM_PROMPT = \"\"\"<Prozess>\n",
    "- Du bist Teil des folgenden Prozesses:\n",
    "    - Um die Ausschließ-URL-Regexes zu bestimmen, wird die Liste der häufigsten URL-Pfade einer Webseite analysiert.\n",
    "    - Diese Liste enthält nur URLs, welche eine bestimmte Mindestanzahl an Unterseiten haben (z. B. mindestens 5), wodurch sie besonders relevant für Ausschließungen sind, da der Verdacht auf strukturierte, CMS Daten nahe liegt.\n",
    "    - Das Ziel ist es, Regexes zu generieren, welche URLs markieren, welche ausgeschlossen werden sollten, da sie anderarbeitig oder gar nicht extrahiert werden sollen.\n",
    "    - Die Regexes sollten präzise sein und nur die gewünschten URL-Gruppen matchen, ohne versehentlich wichtige Seiten auszuschließen.\n",
    "    - Sie sollen aber auch so allgemein wie möglich gehalten werden, und nicht nur auf einzelne Seiten abzielen, sondern immer eher auf die Parent-Pfade, um z. B. alle Detailseiten zu erwischen.\n",
    "    - Regexes sollten wenn möglich eine gesamte URL matchen, also von \"^\" bis \"$\". Oft haben die finalen URLs aber auch noch Queryparameter, deswegen achte auch gerade bei Dateiformaten auf diese, da die Regexes sonst nicht greifen.\n",
    "    - Beispiele für typische Ausschließungen:\n",
    "        - Veranstaltungskalender: \"^.*/(veranstaltungen|termine|events)/.*$\"\n",
    "        - News/Aktuelles: \"^.*/(aktuelles|news|artikel)/.*$\"\n",
    "        - Dateiformate: \"^.*\\\\.(?:pdf|ics|xml|json)(?:\\\\?.*)?$\"\n",
    "        - Archive: \"^.*/(archiv|archive)/.*$\"\n",
    "    - Standardmäßig soll nichts ausgeschlossen werden, auch die obigen Beispiele nicht. Der Nutzer gibt an, was er ausschließen möchte und nur das sollte dann auch ausgeschlossen werden.\n",
    "    - Jedes Regex soll ordentlich begründet werden, um es nachvollziebar zu machen.\n",
    "</Prozess>\"\"\"\n",
    "\n",
    "URL_REGEXES_EXTRACTION_MAIN_SYSTEM_PROMPT = \"\"\"\n",
    "<Häufige URL-Pfade>\n",
    "- Hier sind die häufigsten URL-Pfade mit ihrer Anzahl an Unterseiten:\n",
    "{frequent_urls_text}\n",
    "</Häufige URL-Pfade>\n",
    "\n",
    "<URL Endungen>\n",
    "- Hier sind alle URL Endungen mit ihrer Anzahl:\n",
    "{url_extensions_text}\n",
    "</URL Endungen>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Hier sind die spezifischen Nutzeranweisungen, welche du beachten sollst:\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "## LLM Output models\n",
    "\n",
    "\n",
    "class UrlExclusionRegexSingleResultLlmOutput(BaseModel):\n",
    "    url_regex: str = Field()\n",
    "    reason: str = Field()\n",
    "\n",
    "\n",
    "class UrlExclusionRegexesFullResultLlmOutput(BaseModel):\n",
    "    url_regex_infos: list[UrlExclusionRegexSingleResultLlmOutput] = Field()\n",
    "\n",
    "\n",
    "## System prompt methods\n",
    "\n",
    "\n",
    "def generate_url_regexes_system_prompt(\n",
    "    frequent_sitemap_urls_text: str,\n",
    "    sitema_url_extensions_text: str,\n",
    "    custom_instructions: str,\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(URL_REGEXES_EXTRACTION_SYSTEM_PROMPT)\n",
    "\n",
    "    main_system_prompt = URL_REGEXES_EXTRACTION_MAIN_SYSTEM_PROMPT.format(\n",
    "        frequent_urls_text=frequent_sitemap_urls_text,\n",
    "        url_extensions_text=sitema_url_extensions_text,\n",
    "        custom_instructions=custom_instructions,\n",
    "    )\n",
    "    parts.append(main_system_prompt)\n",
    "\n",
    "    system_prompt = \"\\n\\n\".join(parts)\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "## File helpers\n",
    "\n",
    "### URL Regexes folder\n",
    "\n",
    "\n",
    "def get_url_regexes_folder_path() -> Path:\n",
    "    project_folder_path = PROJECT_MANAGER.get_project_folder_path()\n",
    "    folder_path = project_folder_path / \"10_excluded_urls\"\n",
    "    folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return folder_path\n",
    "\n",
    "\n",
    "### Results folder\n",
    "\n",
    "\n",
    "def get_url_regexes_results_folder_path() -> Path:\n",
    "    url_regexes_folder_path = get_url_regexes_folder_path()\n",
    "    results_folder_path = url_regexes_folder_path / \"results\"\n",
    "    results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return results_folder_path\n",
    "\n",
    "\n",
    "### URL Regexes file\n",
    "\n",
    "\n",
    "def get_url_regexes_file_path() -> Path:\n",
    "    url_regexes_results_folder_path = get_url_regexes_results_folder_path()\n",
    "    file_path = url_regexes_results_folder_path / \"url_regexes.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_url_regexes(url_regexes: list[str]) -> None:\n",
    "    file_path = get_url_regexes_file_path()\n",
    "    regexes_text = \"\\n\".join(url_regexes)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(regexes_text)\n",
    "\n",
    "\n",
    "def get_saved_url_regexes() -> list[str]:\n",
    "    file_path = get_url_regexes_file_path()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        regexes = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return regexes\n",
    "\n",
    "\n",
    "### URL Regexes reason file\n",
    "\n",
    "\n",
    "def get_url_regexes_reason_file_path() -> Path:\n",
    "    url_regexes_results_folder_path = get_url_regexes_results_folder_path()\n",
    "    file_path = url_regexes_results_folder_path / \"url_regexes_reasons.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_url_regexes_reason(\n",
    "    llm_output: UrlExclusionRegexesFullResultLlmOutput,\n",
    ") -> None:\n",
    "    file_path = get_url_regexes_reason_file_path()\n",
    "    reason = llm_output.model_dump_json(indent=2)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(reason)\n",
    "\n",
    "\n",
    "### Excluded URLs file\n",
    "\n",
    "\n",
    "def get_excluded_urls_file_path() -> Path:\n",
    "    url_regexes_results_folder_path = get_url_regexes_results_folder_path()\n",
    "    file_path = url_regexes_results_folder_path / \"excluded_urls.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_excluded_urls(urls: list[str]) -> None:\n",
    "    file_path = get_excluded_urls_file_path()\n",
    "    urls_text = \"\\n\".join(urls)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(urls_text)\n",
    "\n",
    "\n",
    "def get_saved_excluded_urls() -> list[str]:\n",
    "    file_path = get_excluded_urls_file_path()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        urls = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return urls\n",
    "\n",
    "\n",
    "### Non-excluded URLs file\n",
    "\n",
    "\n",
    "def get_non_excluded_urls_file_path() -> Path:\n",
    "    url_regexes_results_folder_path = get_url_regexes_results_folder_path()\n",
    "    file_path = url_regexes_results_folder_path / \"non_excluded_urls.txt\"\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def save_non_excluded_urls(urls: list[str]) -> None:\n",
    "    file_path = get_non_excluded_urls_file_path()\n",
    "    urls_text = \"\\n\".join(urls)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(urls_text)\n",
    "\n",
    "\n",
    "def get_saved_non_excluded_urls() -> list[str]:\n",
    "    file_path = get_non_excluded_urls_file_path()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        urls = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return urls\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "async def extract_url_regexes(custom_instructions: str) -> None:\n",
    "    log_message = \"Extracting URL regexes...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    frequent_sitemap_urls_text = get_saved_frequent_sitemap_urls()\n",
    "\n",
    "    sitemap_url_extensions_text = get_saved_sitemap_url_extensions()\n",
    "\n",
    "    log_message = \"Analyzing frequent URL paths...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    system_prompt = generate_url_regexes_system_prompt(\n",
    "        frequent_sitemap_urls_text, sitemap_url_extensions_text, custom_instructions\n",
    "    )\n",
    "    full_result_llm_output = await call_structured_llm(\n",
    "        system_prompt,\n",
    "        UrlExclusionRegexesFullResultLlmOutput,\n",
    "    )\n",
    "    url_regexes = [\n",
    "        x_regex_info.url_regex\n",
    "        for x_regex_info in full_result_llm_output.url_regex_infos\n",
    "    ]\n",
    "\n",
    "    log_message = f\"Found {len(url_regexes)} URL regexes.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    save_url_regexes(url_regexes)\n",
    "    save_url_regexes_reason(full_result_llm_output)\n",
    "\n",
    "    log_message = f\"Extracted {len(url_regexes)} URL regexes.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    log_message = \"Applying URL regexes to sitemap URLs...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    all_sitemap_urls = get_saved_sitemap_urls()\n",
    "\n",
    "    excluded_urls: list[str] = []\n",
    "    non_excluded_urls: list[str] = []\n",
    "\n",
    "    for x_url in all_sitemap_urls:\n",
    "        is_excluded = False\n",
    "        for y_regex in url_regexes:\n",
    "            if re.match(y_regex, x_url):\n",
    "                is_excluded = True\n",
    "                break\n",
    "        if is_excluded:\n",
    "            excluded_urls.append(x_url)\n",
    "        else:\n",
    "            non_excluded_urls.append(x_url)\n",
    "\n",
    "    log_message = f\"Applied URL regexes. Found {len(excluded_urls)} excluded and {len(non_excluded_urls)} non-excluded URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    save_excluded_urls(excluded_urls)\n",
    "    save_non_excluded_urls(non_excluded_urls)\n",
    "\n",
    "    log_message = \"Saved excluded and non-excluded URLs.\"\n",
    "    logger.info(log_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "536eb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting URLs\n",
    "\n",
    "## Prompts\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_BATCH_SIZE = 500\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_COMMON_SYSTEM_PROMPT = \"\"\"<Prozess>\n",
    "- Du bist Teil des folgenden Prozesses:\n",
    "    - Um die CSS-Ausschließ-Selektoren zu bestimmen, müssen einige Sample Seiten der Webseite analysiert werden und auf diesen dann die CSS-Selektoren angewendet werden um zu schauen, ob sie den gewünschten Effekt haben.\n",
    "    - Dazu werden zu erst aus der Sitemap einer Webseite interessante, diverse URLs ausgewählt, welche das Sample Set darstellen.\n",
    "    - Dabei sollten diese Seiten besonders repräsentativ für die gesamte Webseite sein. Z. B. einmal die Startseite, dann eine Veranstaltungs-Übersichts-Seite, eine Veranstaltungs-Detail-Seite, eine News-Übersichts-Seite, eine News-Detail-Seite, eine Blog-Übersichts-Seite, eine Blog-Detail-Seite, eine Archivseite, eine Kontaktseite, eine Impressumseite, etc. Einfach die verschiedensten Datenstrukturen, Formate und Inhalte\n",
    "    - Also ein Set an Seiten, bei dem wir auch unterschiedliche Inhalte und Layoutstrukturen erwarten.\n",
    "    - Natürlich können wir das nicht genau wissen, da wir nur die URLs sehen und aus diesen einfach von außen auswählen müssen. Tortzdem lässt sich an den URLs und Pfadsegmenten schon sehr gut ablesen, welche Seiten unterschiedliche Inhalte enthalten sollten.\n",
    "    - Da eine Webseite tausende Seiten enthalten kann, gehen wir hierbei in Batches vor. Zuerst extrahieren mehrere KI-Agenten aus jeweils 500 URLs ein Sample Set und begründen ihre Auswahlen. Es ist wichtig eine gute Begründung zu geben, damit der zusammenfassende KI-Agent die Gedanken hinter den Auswahlen besser versteht.\n",
    "    - Dann nimmt ein zweiter KI-Agent die Batches und kombiniert diese zu einem finalen Sample Set, indem er versucht die besten URLs auszuwählen. Dabei versucht er auf maximal 15 URLs zu kommen.\n",
    "    - Das Sample Set wird dann später heruntergeladen und vom Nutzer analysiert.\n",
    "</Prozess>\"\"\"\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_BATCH_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welcher die Auswahl der URLs für das Sample Set vornimmt und dabei einen Batch von maximal 500 URLs bearbeitet. Du bist also nicht der, welcher am Ende die ganzen Batches zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<URLs>\n",
    "- Hier sind die URLs, aus welchen du ein Sample Set auswählen sollst:\n",
    "{urls_text}\n",
    "</URLs>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer die ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welche die ganzen Batches zu einem finalen Sample Set zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<Batch Sample Sets>\n",
    "- Hier sind die Batches, welche du zusammenfassen sollst:\n",
    "{batch_llm_outputs_text}\n",
    "</Batch Sample Sets>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer die ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "## LLM Output models\n",
    "\n",
    "\n",
    "class InterestingUrlsSingleResultlLlmOutput(BaseModel):\n",
    "    url: str = Field()\n",
    "    reason: str = Field()\n",
    "\n",
    "\n",
    "class InterestingUrlsFullResultLlmOutput(BaseModel):\n",
    "    url_infos: list[InterestingUrlsSingleResultlLlmOutput] = Field()\n",
    "\n",
    "\n",
    "## System prompt methods\n",
    "\n",
    "\n",
    "def generate_interesting_urls_batch_system_prompt(\n",
    "    urls: list[str], custom_instructions: str\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(INTERESTING_URLS_EXTRACTION_COMMON_SYSTEM_PROMPT)\n",
    "\n",
    "    urls_text = \"\\n\".join(\n",
    "        [f\"{x_index + 1}. {x_url}\" for x_index, x_url in enumerate(urls)]\n",
    "    )\n",
    "    batch_system_prompt = INTERESTING_URLS_EXTRACTION_BATCH_SYSTEM_PROMPT.format(\n",
    "        urls_text=urls_text, custom_instructions=custom_instructions\n",
    "    )\n",
    "    parts.append(batch_system_prompt)\n",
    "\n",
    "    system_prompt = \"\\n\\n\".join(parts)\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "def generate_interesting_urls_summarizer_system_prompt(\n",
    "    batch_full_result_llm_outputs: list[InterestingUrlsFullResultLlmOutput],\n",
    "    custom_instructions: str,\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT)\n",
    "\n",
    "    batch_llm_outputs_text = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Batch {x_index + 1}:\\n\"\n",
    "            + \"\\n\".join(\n",
    "                [\n",
    "                    f\"- {y_single_result_llm_output.url}\\n  Reason: {y_single_result_llm_output.reason}\"\n",
    "                    for y_single_result_llm_output in x_batch_full_resultllm_output.url_infos\n",
    "                ]\n",
    "            )\n",
    "            for x_index, x_batch_full_resultllm_output in enumerate(\n",
    "                batch_full_result_llm_outputs\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    system_prompt = INTERESTING_URLS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT.format(\n",
    "        batch_llm_outputs_text=batch_llm_outputs_text,\n",
    "        custom_instructions=custom_instructions,\n",
    "    )\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "## File helpers\n",
    "\n",
    "### Interesting URLs folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_folder_path() -> Path:\n",
    "    project_folder_path = PROJECT_MANAGER.get_project_folder_path()\n",
    "    file_path = project_folder_path / \"20_interesting_urls\"\n",
    "    file_path.mkdir(exist_ok=True, parents=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "### Results folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_results_folder_path() -> Path:\n",
    "    interesting_urls_folder_path = get_interesting_urls_folder_path()\n",
    "    results_folder_path = interesting_urls_folder_path / \"results\"\n",
    "    results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return results_folder_path\n",
    "\n",
    "\n",
    "### Interesting URLs file\n",
    "\n",
    "\n",
    "def save_interesting_urls(urls: list[str]) -> None:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    file_path = interesting_urls_results_folder_path / \"interesting_urls.txt\"\n",
    "    urls_text = \"\\n\".join(urls)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(urls_text)\n",
    "\n",
    "\n",
    "def get_interesting_urls() -> list[str]:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    interesting_urls_file_path = (\n",
    "        interesting_urls_results_folder_path / \"interesting_urls.txt\"\n",
    "    )\n",
    "    with open(interesting_urls_file_path, \"r\") as file:\n",
    "        urls = [\n",
    "            x_line.strip() for x_line in file.readlines() if len(x_line.strip()) > 0\n",
    "        ]\n",
    "    return urls\n",
    "\n",
    "\n",
    "### Interesting URLs reason file\n",
    "\n",
    "\n",
    "def save_interesting_urls_reason(\n",
    "    llm_output: InterestingUrlsFullResultLlmOutput,\n",
    ") -> None:\n",
    "    interesting_urls_results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    file_path = interesting_urls_results_folder_path / \"interesting_urls_reasons.txt\"\n",
    "    reason = llm_output.model_dump_json(indent=2)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(reason)\n",
    "\n",
    "\n",
    "### Interesting URLs downloads folder\n",
    "\n",
    "\n",
    "def get_interesting_urls_downloads_folder_path() -> Path:\n",
    "    interesting_urls_folder_path = get_interesting_urls_folder_path()\n",
    "    downloaded_folder_path = interesting_urls_folder_path / \"downloads\"\n",
    "    downloaded_folder_path.mkdir(exist_ok=True)\n",
    "    return downloaded_folder_path\n",
    "\n",
    "\n",
    "### Interesting URLs downloads file\n",
    "\n",
    "\n",
    "def save_downloaded_url(index: int, url: str, html_content: str) -> None:\n",
    "    downloads_folder_path = get_interesting_urls_downloads_folder_path()\n",
    "    index_file_name_part = f\"{index:03d}\"\n",
    "    url_file_name_part = convert_url_to_file_name(url)\n",
    "    full_file_name = f\"{index_file_name_part}_{url_file_name_part}.html\"\n",
    "    file_path = downloads_folder_path / full_file_name\n",
    "    with open(file_path) as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "async def download_interesting_urls() -> None:\n",
    "    log_message = \"Downloading interesting URLs...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    urls = get_interesting_urls()\n",
    "\n",
    "    log_message = f\"Found {len(urls)} URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=60) as http_client:\n",
    "        download_tasks = [http_client.get(x_url) for x_url in urls]\n",
    "        responses = await asyncio.gather(*download_tasks)\n",
    "\n",
    "    sub_state_data_packs: list[SubStateDataPack] = []\n",
    "\n",
    "    for x_url, x_response in zip(urls, responses):\n",
    "        soup = BeautifulSoup(x_response.text, \"html.parser\")\n",
    "        prettified_html = soup.prettify()\n",
    "\n",
    "        sub_state_data_pack = SubStateDataPack(\n",
    "            url=x_url,\n",
    "            raw_html=prettified_html,\n",
    "            cleaned_html=\"\",\n",
    "            raw_markdown=\"\",\n",
    "            cleaned_markdown=\"\",\n",
    "            feedback=\"\",\n",
    "        )\n",
    "        sub_state_data_packs.append(sub_state_data_pack)\n",
    "\n",
    "    results_folder_path = get_interesting_urls_results_folder_path()\n",
    "    downloads_folder_path = get_interesting_urls_downloads_folder_path()\n",
    "    sub_state = SubState(\n",
    "        versions_folder_path=downloads_folder_path,\n",
    "        cloneable_result_folder_path=results_folder_path,\n",
    "        sub_state_data_packs=sub_state_data_packs,\n",
    "    )\n",
    "    SUB_STATE_MANAGER.save_sub_state(sub_state)\n",
    "\n",
    "    log_message = f\"Downloaded {len(urls)} URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "\n",
    "async def extract_interesting_urls(custom_instructions: str) -> None:\n",
    "    log_message = \"Extracting interesting URLs...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    non_excluded_urls = get_saved_non_excluded_urls()\n",
    "\n",
    "    log_message = f\"Found {len(non_excluded_urls)} non-excluded URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    url_batches = batch_items(non_excluded_urls, INTERESTING_URLS_EXTRACTION_BATCH_SIZE)\n",
    "    batch_full_result_llm_outputs: list[InterestingUrlsFullResultLlmOutput] = []\n",
    "\n",
    "    batch_system_prompts = [\n",
    "        generate_interesting_urls_batch_system_prompt(x_url_batch, custom_instructions)\n",
    "        for x_url_batch in url_batches\n",
    "    ]\n",
    "    batch_full_result_llm_outputs = await call_structured_llm_batch(\n",
    "        batch_system_prompts,\n",
    "        InterestingUrlsFullResultLlmOutput,\n",
    "    )\n",
    "\n",
    "    log_message = f\"Summarizing {len(batch_full_result_llm_outputs)} batches...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    system_prompt = generate_interesting_urls_summarizer_system_prompt(\n",
    "        batch_full_result_llm_outputs, custom_instructions\n",
    "    )\n",
    "    summarized_full_result_llm_output = await call_structured_llm(\n",
    "        system_prompt,\n",
    "        InterestingUrlsFullResultLlmOutput,\n",
    "    )\n",
    "    urls = [\n",
    "        x_url_info.url for x_url_info in summarized_full_result_llm_output.url_infos\n",
    "    ]\n",
    "\n",
    "    log_message = f\"Summarized {len(batch_full_result_llm_outputs)} batches and found {len(urls)} interesting URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    save_interesting_urls(urls)\n",
    "    save_interesting_urls_reason(summarized_full_result_llm_output)\n",
    "\n",
    "    log_message = f\"Extracted {len(urls)} interesting URLs.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    await download_interesting_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0af039f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS selectors\n",
    "\n",
    "## Prompts\n",
    "\n",
    "CSS_SELECTORS_EXTRACTION_SYSTEM_PROMPT = \"\"\"<Prozess>\n",
    "- Du bist Teil des folgenden Prozesses:\n",
    "    - Um die Ausschließ-CSS-Selektoren zu bestimmen, wird ein Sample Set an Unterseiten der Webseite analysiert.\n",
    "    - Dabei werden die heruntergeladenen HTML-Seiten betrachtet und Ausschließ-CSS-Selektoren identifiziert, die \"Verschmutzungen\" entfernen (Header, Footer, Cookiebanner, Werbung, etc.).\n",
    "    - Dabei soll alles entfernt werden, was nicht wirklichen Inhalt darstellt. Natürlich sollen Titel, Beschreibungen, Kontaktdaten, Öffnungszeiten, FAQ Akkordions, etc. alles bleiben.\n",
    "    - Aber viel bei einer Webseite ist auch einfach um die eigentlichen Inhalte \"drumherum\", z. B. Nav, Footer, PopUps, Bedienungshilfen, Socialmedia Widgets, Breadcrumbs, etc. diese sollen alle entfernt werden.\n",
    "    - Das Ziel ist es, CSS-Selektoren zu finden, welche auf allen Seiten anwendbar sind, da am Ende das gleiche Set für alle tausende Seiten der Webseite verwendet wird.\n",
    "    - CSS-Selektoren sollten so generisch wie möglich gehalten werden und z. B. nicht auf bestimmte Titel auf bestimmten Seiten abzielen.\n",
    "    - Es sollte auch immer das höchstmögliche Element targetiert werden, z. B. sollte natürlich nicht jeder Button einzelnd in einem Cookie-Banner entfernt werden, sondern direkt der ganze Banner oder wenn dieser im Footer ist, welcher auch weg soll, dann direkt der gesamte Footer. So minimieren wir die benötigte Anzahl an CSS-Selektoren.\n",
    "    - Da mit etwa 20 Sample Seiten gearbeitet wird und das rohe HTML noch sehr lang ist, wird in Batches vorgegangen. Zuerst analysieren mehrere KI-Agenten immer 1 Seite und identifizieren perfekte CSS-Selektoren und begründen auch ihre Auswahl. Es ist wichtig eine gute Begründung zu geben, damit der zusammenfassende KI-Agent die Gedanken hinter den Auswahlen besser versteht. Zusätzlich wird zu jedem CSS-Selektor auch eine Beispiel-Zeilennummer aus dem HTML angegeben. Durch diese wird dann ein Beispiel-Code-Block aus dem HTML geschnitten und auch dem zusammenfassenden KI-Agenten gegeben, damit dieser die Entscheidungen besser nachvollziehen kann und auch beim Kombinieren noch selbst eine gute Entscheidungsgrundlage hat.\n",
    "    - Dann nimmt ein zweiter KI-Agent die Batches und kombiniert diese zu einer finalen Liste an CSS-Selektoren, indem er versucht die CSS-Selektoren so generisch wie möglich zu kombinieren und versucht sicherzustellen, dass die CSS-Selektoren auf allen Seiten anwendbar sind. Dabei gibt es keine Limitierung für die Anzahl an CSS-Selektoren.\n",
    "    - Diese Selektoren werden dann später in neuracrawl verwendet, um sauberes Markdown zu extrahieren.\n",
    "</Prozess>\"\"\"\n",
    "\n",
    "CSS_SELECTORS_EXTRACTION_BATCH_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welcher die CSS-Selektoren für eine einzelne HTML-Seite identifiziert. Du bist also nicht der, welcher am Ende die ganzen Batches zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<HTML>\n",
    "- Hier ist die HTML-Seite, welche du analysieren sollst (mit Zeilennummern):\n",
    "{html_text}\n",
    "</HTML>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer dir ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "CSS_SELECTORS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT = \"\"\"<Aufgabe>\n",
    "- Genauer gesagt, bist du der KI-Agent, welcher die ganzen Batches zu einer finalen Liste an CSS-Selektoren zusammenfasst.\n",
    "</Aufgabe>\n",
    "\n",
    "<Batch CSS Selectors>\n",
    "- Hier sind die Batches, welche du zusammenfassen sollst:\n",
    "{batch_llm_outputs_text}\n",
    "</Batch CSS Selectors>\n",
    "\n",
    "<Zusatzanweisungen>\n",
    "- Eventuell gibt der Nutzer dir ein paar Zusatzanweisungen, um dich etwas mehr zu leiten. Die ursprüngliche Aufgabe bleibt, aber die Zusatzanweisungen können dir helfen, eine Auswahl zu treffen, welche mehr den Vorstellungen des Nutzers entspricht.\n",
    "{custom_instructions}\n",
    "</Zusatzanweisungen>\"\"\"\n",
    "\n",
    "## LLM Output models\n",
    "\n",
    "\n",
    "class CssSelectorSingleResultLlmOutput(BaseModel):\n",
    "    css_selector: str = Field()\n",
    "    reason: str = Field()\n",
    "    example_line_number: int = Field()\n",
    "\n",
    "\n",
    "class CssSelectorsFullResultLlmOutput(BaseModel):\n",
    "    css_selector_infos: list[CssSelectorSingleResultLlmOutput] = Field()\n",
    "\n",
    "\n",
    "## Models\n",
    "\n",
    "\n",
    "class ExtendedCssSelectorSingleResult(CssSelectorSingleResultLlmOutput):\n",
    "    example_html: str = Field()\n",
    "\n",
    "\n",
    "class ExtendedCssSelectorsFullResult(BaseModel):\n",
    "    url: str = Field()\n",
    "    css_selector_infos: list[ExtendedCssSelectorSingleResult] = Field()\n",
    "\n",
    "\n",
    "## System prompt methods\n",
    "\n",
    "\n",
    "def generate_css_selectors_batch_system_prompt(\n",
    "    html_content: str, custom_instructions: str\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(CSS_SELECTORS_EXTRACTION_SYSTEM_PROMPT)\n",
    "\n",
    "    # Add line numbers to HTML\n",
    "    html_lines = html_content.split(\"\\n\")\n",
    "    numbered_html_lines = [\n",
    "        f\"{x_index + 1:5d} | {x_line}\" for x_index, x_line in enumerate(html_lines)\n",
    "    ]\n",
    "    html_text = \"\\n\".join(numbered_html_lines)\n",
    "    batch_system_prompt = CSS_SELECTORS_EXTRACTION_BATCH_SYSTEM_PROMPT.format(\n",
    "        html_text=html_text, custom_instructions=custom_instructions\n",
    "    )\n",
    "    parts.append(batch_system_prompt)\n",
    "\n",
    "    system_prompt = \"\\n\\n\".join(parts)\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "def generate_css_selectors_summarizer_system_prompt(\n",
    "    batch_llm_outputs: list[ExtendedCssSelectorsFullResult],\n",
    "    custom_instructions: str,\n",
    ") -> str:\n",
    "    parts: list[str] = []\n",
    "    parts.append(GENERAL_SYSTEM_PROMPT)\n",
    "    parts.append(CSS_SELECTORS_EXTRACTION_SYSTEM_PROMPT)\n",
    "\n",
    "    batch_llm_outputs_text = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Batch {x_index + 1} (URL: {x_batch_llm_output.url}):\\n\"\n",
    "            + \"\\n\".join(\n",
    "                [\n",
    "                    f\"- CSS Selector: {y_selector_output.css_selector}\\n  Reason: {y_selector_output.reason}\\n  Example HTML:\\n{y_selector_output.example_html}\"\n",
    "                    for y_selector_output in x_batch_llm_output.css_selector_infos\n",
    "                ]\n",
    "            )\n",
    "            for x_index, x_batch_llm_output in enumerate(batch_llm_outputs)\n",
    "        ]\n",
    "    )\n",
    "    system_prompt = CSS_SELECTORS_EXTRACTION_SUMMARIZER_SYSTEM_PROMPT.format(\n",
    "        batch_llm_outputs_text=batch_llm_outputs_text,\n",
    "        custom_instructions=custom_instructions,\n",
    "    )\n",
    "    parts.append(system_prompt)\n",
    "\n",
    "    system_prompt = \"\\n\\n\".join(parts)\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "## File helpers\n",
    "\n",
    "### CSS selectors folder\n",
    "\n",
    "\n",
    "def get_css_selectors_folder_path() -> Path:\n",
    "    project_folder_path = PROJECT_MANAGER.get_project_folder_path()\n",
    "    file_path = project_folder_path / \"30_css_selectors\"\n",
    "    file_path.mkdir(exist_ok=True, parents=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "### Results folder\n",
    "\n",
    "\n",
    "def get_css_selectors_results_folder_path() -> Path:\n",
    "    css_selectors_folder_path = get_css_selectors_folder_path()\n",
    "    results_folder_path = css_selectors_folder_path / \"results\"\n",
    "    results_folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    return results_folder_path\n",
    "\n",
    "\n",
    "### CSS selectors file\n",
    "\n",
    "\n",
    "def save_css_selectors(css_selectors: list[str]) -> None:\n",
    "    css_selectors_results_folder_path = get_css_selectors_results_folder_path()\n",
    "    file_path = css_selectors_results_folder_path / \"css_selectors.txt\"\n",
    "    selectors_text = \"\\n\".join(css_selectors)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(selectors_text)\n",
    "\n",
    "\n",
    "### CSS selectors reason file\n",
    "\n",
    "\n",
    "def save_css_selectors_reason(\n",
    "    llm_output: CssSelectorsFullResultLlmOutput,\n",
    ") -> None:\n",
    "    css_selectors_results_folder_path = get_css_selectors_results_folder_path()\n",
    "    file_path = css_selectors_results_folder_path / \"css_selectors_reasons.txt\"\n",
    "    reason = llm_output.model_dump_json(indent=2)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(reason)\n",
    "\n",
    "\n",
    "### CSS selectors downloads folder\n",
    "\n",
    "\n",
    "def get_css_selectors_downloads_folder_path() -> Path:\n",
    "    css_selectors_folder_path = get_css_selectors_folder_path()\n",
    "    downloads_folder_path = css_selectors_folder_path / \"downloads\"\n",
    "    downloads_folder_path.mkdir(exist_ok=True)\n",
    "    return downloads_folder_path\n",
    "\n",
    "\n",
    "## Utilities\n",
    "\n",
    "\n",
    "# AIGENERATED\n",
    "def extract_example_html_lines(html_content: str, line_number: int) -> str:\n",
    "    html_lines = html_content.split(\"\\n\")\n",
    "    start_index = max(\n",
    "        0, line_number - 4\n",
    "    )  # -4 because line_number is 1-indexed and we want 3 lines before\n",
    "    end_index = min(len(html_lines), line_number + 3)  # +3 to get 3 lines after\n",
    "\n",
    "    example_lines = html_lines[start_index:end_index]\n",
    "    numbered_example_lines = [\n",
    "        f\"{start_index + x_index + 1:5d} | {x_line}\"\n",
    "        for x_index, x_line in enumerate(example_lines)\n",
    "    ]\n",
    "    return \"\\n\".join(numbered_example_lines)\n",
    "\n",
    "\n",
    "def hydrate_full_results_to_extended_full_results(\n",
    "    full_results: list[CssSelectorsFullResultLlmOutput],\n",
    "    sub_state_data_packs: list[SubStateDataPack],\n",
    ") -> list[ExtendedCssSelectorsFullResult]:\n",
    "    extended_full_results: list[ExtendedCssSelectorsFullResult] = []\n",
    "\n",
    "    for x_sub_state_data_pack, x_llm_output in zip(sub_state_data_packs, full_results):\n",
    "        extended_single_results: list[ExtendedCssSelectorSingleResult] = []\n",
    "\n",
    "        for x_css_selector_info in x_llm_output.css_selector_infos:\n",
    "            example_html = extract_example_html_lines(\n",
    "                x_sub_state_data_pack.cleaned_html,\n",
    "                x_css_selector_info.example_line_number,\n",
    "            )\n",
    "            extended_single_result = ExtendedCssSelectorSingleResult(\n",
    "                css_selector=x_css_selector_info.css_selector,\n",
    "                reason=x_css_selector_info.reason,\n",
    "                example_line_number=x_css_selector_info.example_line_number,\n",
    "                example_html=example_html,\n",
    "            )\n",
    "            extended_single_results.append(extended_single_result)\n",
    "\n",
    "        extended_full_result = ExtendedCssSelectorsFullResult(\n",
    "            url=x_sub_state_data_pack.url,\n",
    "            css_selector_infos=extended_single_results,\n",
    "        )\n",
    "        extended_full_results.append(extended_full_result)\n",
    "\n",
    "    return extended_full_results\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "\n",
    "async def extract_css_selectors(\n",
    "    custom_instructions: str, interesting_urls_save_state_version: str | None = None\n",
    ") -> None:\n",
    "    log_message = \"Extracting CSS selectors...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    interesting_urls_downloads_folder_path = (\n",
    "        get_interesting_urls_downloads_folder_path()\n",
    "    )\n",
    "    interesting_urls_sub_state = SUB_STATE_MANAGER.get_sub_state(\n",
    "        interesting_urls_downloads_folder_path, interesting_urls_save_state_version\n",
    "    )\n",
    "    sub_state_data_packs = interesting_urls_sub_state.sub_state_data_packs\n",
    "\n",
    "    log_message = f\"Found {len(sub_state_data_packs)} pages.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    log_message = f\"Pre-cleaning HTML for {len(sub_state_data_packs)} pages...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    precleaned_sub_state_data_packs: list[SubStateDataPack] = []\n",
    "\n",
    "    for x_sub_state_data_pack in sub_state_data_packs:\n",
    "        precleaned_html = CLEANER.preclean_html(x_sub_state_data_pack.raw_html)\n",
    "        precleaned_sub_state_data_pack = x_sub_state_data_pack.model_copy()\n",
    "        precleaned_sub_state_data_pack.cleaned_html = precleaned_html\n",
    "        precleaned_sub_state_data_packs.append(precleaned_sub_state_data_pack)\n",
    "\n",
    "    log_message = f\"Pre-cleaned HTML for {len(precleaned_sub_state_data_packs)} pages.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    batch_system_prompts = [\n",
    "        generate_css_selectors_batch_system_prompt(\n",
    "            x_sub_state_data_pack.cleaned_html, custom_instructions\n",
    "        )\n",
    "        for x_sub_state_data_pack in precleaned_sub_state_data_packs\n",
    "    ]\n",
    "    for x_prompt in batch_system_prompts:\n",
    "        print(len(x_prompt))\n",
    "    batch_full_results = await call_structured_llm_batch(\n",
    "        batch_system_prompts,\n",
    "        CssSelectorsFullResultLlmOutput,\n",
    "    )\n",
    "    extended_batch_full_results = hydrate_full_results_to_extended_full_results(\n",
    "        batch_full_results, precleaned_sub_state_data_packs\n",
    "    )\n",
    "\n",
    "    log_message = f\"Summarizing {len(extended_batch_full_results)} batches...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    system_prompt = generate_css_selectors_summarizer_system_prompt(\n",
    "        extended_batch_full_results, custom_instructions\n",
    "    )\n",
    "    summarized_full_result = await call_structured_llm(\n",
    "        system_prompt,\n",
    "        CssSelectorsFullResultLlmOutput,\n",
    "    )\n",
    "    css_selectors = [\n",
    "        x_selector_output.css_selector\n",
    "        for x_selector_output in summarized_full_result.css_selector_infos\n",
    "    ]\n",
    "\n",
    "    log_message = f\"Summarized {len(extended_batch_full_results)} batches and found {len(css_selectors)} CSS selectors.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    save_css_selectors(css_selectors)\n",
    "    save_css_selectors_reason(summarized_full_result)\n",
    "\n",
    "    log_message = f\"Processing {len(sub_state_data_packs)} pages...\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    processed_sub_state_data_packs: list[SubStateDataPack] = []\n",
    "    for x_sub_state_data_pack in sub_state_data_packs:\n",
    "        cleaned_html = CLEANER.clean_html(x_sub_state_data_pack.raw_html, css_selectors)\n",
    "        raw_markdown = CLEANER.convert_html_to_markdown(cleaned_html)\n",
    "        cleaned_markdown = CLEANER.clean_markdown(raw_markdown)\n",
    "\n",
    "        processed_sub_state_data_pack = x_sub_state_data_pack.model_copy()\n",
    "        processed_sub_state_data_pack.cleaned_html = cleaned_html\n",
    "        processed_sub_state_data_pack.raw_markdown = raw_markdown\n",
    "        processed_sub_state_data_pack.cleaned_markdown = cleaned_markdown\n",
    "        processed_sub_state_data_packs.append(processed_sub_state_data_pack)\n",
    "\n",
    "    log_message = f\"Processed {len(processed_sub_state_data_packs)} pages.\"\n",
    "    logger.info(log_message)\n",
    "\n",
    "    downloads_folder_path = get_css_selectors_downloads_folder_path()\n",
    "    results_folder_path = get_css_selectors_results_folder_path()\n",
    "    sub_state = SubState(\n",
    "        versions_folder_path=downloads_folder_path,\n",
    "        cloneable_result_folder_path=results_folder_path,\n",
    "        sub_state_data_packs=processed_sub_state_data_packs,\n",
    "    )\n",
    "    SUB_STATE_MANAGER.save_sub_state(sub_state)\n",
    "\n",
    "    log_message = f\"Extracted {len(css_selectors)} CSS selectors.\"\n",
    "    logger.info(log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "547231af",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Setup done.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[235]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSetup done.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Setup done."
     ]
    }
   ],
   "source": [
    "raise Exception(\"Setup done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd989b",
   "metadata": {},
   "source": [
    "# neuracrawl Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "526791d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your project name here\n",
    "\n",
    "PROJECT_MANAGER.set_project(\"siegburg\")\n",
    "create_sitemap_urls_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c957d",
   "metadata": {},
   "source": [
    "## Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sitemap_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 169 common URL areas with frequency >= 5\n"
     ]
    }
   ],
   "source": [
    "extract_frequent_sitemap_urls(5)\n",
    "extract_url_extensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe84d31",
   "metadata": {},
   "source": [
    "## Exclusion URL Regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "886b2c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracting URL regexes...\n",
      "INFO:__main__:Analyzing frequent URL paths...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Found 3 URL regexes.\n",
      "INFO:__main__:Extracted 3 URL regexes.\n",
      "INFO:__main__:Applying URL regexes to sitemap URLs...\n",
      "INFO:__main__:Applied URL regexes. Found 5097 excluded and 1805 non-excluded URLs.\n",
      "INFO:__main__:Saved excluded and non-excluded URLs.\n"
     ]
    }
   ],
   "source": [
    "await extract_url_regexes(\n",
    "    \"Exclude all events and news. Also exclude all pdf, json, xml, ics, vcf.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af9296",
   "metadata": {},
   "source": [
    "## Interesting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b9df22df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracting interesting URLs...\n",
      "INFO:__main__:Found 1805 non-excluded URLs.\n",
      "INFO:__main__:Processing prompt (1/4)...\n",
      "INFO:__main__:Processing prompt (2/4)...\n",
      "INFO:__main__:Processing prompt (3/4)...\n",
      "INFO:__main__:Processing prompt (4/4)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (3/4).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (2/4).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (4/4).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (1/4).\n",
      "INFO:__main__:Summarizing 4 batches...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Summarized 4 batches and found 31 interesting URLs.\n",
      "INFO:__main__:Extracted 31 interesting URLs.\n",
      "INFO:__main__:Downloading interesting URLs...\n",
      "INFO:__main__:Found 31 URLs.\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/stadtportraet/livebilder/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/orte/sehenswuerdigkeiten/grabsteine/grabstein-001-belin-denis/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/orte/sehenswuerdigkeiten/abtei-michaelsberg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/familie-bildung-soziales/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/themen/faq-zur-grundsteuer/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/wirtschaft/veranstaltungen/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/service-verwaltung/wahlen/kommunalwahl/kommunalwahl-2020/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/abteilungen/NRW:department:15352/amt-13-amt-fuer-kommunikation-und-historisches-archiv/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/impressum/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/feuerwehr-siegburg/organisation/freiwillige-feuerwehr/loeschgruppe-1/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/cityportal/gastronomie/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/umwelt-klimaschutz/klimaschutz-klimanpassung/soziale-klimaquartiere-siegburg/soziales-klimaquartier-deichhaus/energieforum-deichhaus-2024/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/mitarbeiter/NRW:employee:62386/glathe-carsten/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/cityportal/firmen-unternehmen/absolut-designhaus/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/orte/sehenswuerdigkeiten/stolpersteine/familie-bock/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/personen/eikermann-alexander/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/cityportal/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/barrierefreiheit/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/cityportal/vereine/aikido-club-siegburg/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/sitemap/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/leistungen/NRW:entry:40417-VLR/baugenehmigung-beantragen/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/leistungen/NRW:entry:42373-VLR/terminreservierung-online/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/serviceportal-a-z/dienstleistungen/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/kalender/kombinierter-kalender/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/abteilungen/NRW:department:15268/dezernat-i/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/service-verwaltung/rathaus/stellenangebote-ausbildung/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/buergerservice-views/leistungen/NRW:entry:40015-VLR/bewohnerparkausweis/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/stadtleben-aktuelles/stadtportraet/stadtgeschichte/siegburger-alben/ausstellung-01/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/strukturierte-daten/kindertagesstaetten/die-deichmaeuse/ \"HTTP/1.1 200 200\"\n",
      "INFO:httpx:HTTP Request: GET https://siegburg.de/planen-bauen-verkehr/planen-bauen/bauvorhaben/rathaussanierung/ \"HTTP/1.1 200 200\"\n",
      "INFO:__main__:Downloaded 31 URLs.\n"
     ]
    }
   ],
   "source": [
    "await extract_interesting_urls(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1fc1b",
   "metadata": {},
   "source": [
    "## CSS Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "11406201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracting CSS selectors...\n",
      "INFO:__main__:Found 31 pages.\n",
      "INFO:__main__:Pre-cleaning HTML for 31 pages...\n",
      "INFO:__main__:Pre-cleaned HTML for 31 pages.\n",
      "234074\n",
      "64039\n",
      "52034\n",
      "53221\n",
      "67977\n",
      "59398\n",
      "50559\n",
      "49024\n",
      "101580\n",
      "50192\n",
      "75933\n",
      "40660\n",
      "222053\n",
      "97616\n",
      "80701\n",
      "49340\n",
      "63965\n",
      "139102\n",
      "51127\n",
      "50541\n",
      "48882\n",
      "58644\n",
      "63904\n",
      "58479\n",
      "48766\n",
      "56182\n",
      "48941\n",
      "50121\n",
      "47943\n",
      "95207\n",
      "46614\n",
      "INFO:__main__:Processing prompt (1/31)...\n",
      "INFO:__main__:Processing prompt (2/31)...\n",
      "INFO:__main__:Processing prompt (3/31)...\n",
      "INFO:__main__:Processing prompt (4/31)...\n",
      "INFO:__main__:Processing prompt (5/31)...\n",
      "INFO:__main__:Processing prompt (6/31)...\n",
      "INFO:__main__:Processing prompt (7/31)...\n",
      "INFO:__main__:Processing prompt (8/31)...\n",
      "INFO:__main__:Processing prompt (9/31)...\n",
      "INFO:__main__:Processing prompt (10/31)...\n",
      "INFO:__main__:Processing prompt (11/31)...\n",
      "INFO:__main__:Processing prompt (12/31)...\n",
      "INFO:__main__:Processing prompt (13/31)...\n",
      "INFO:__main__:Processing prompt (14/31)...\n",
      "INFO:__main__:Processing prompt (15/31)...\n",
      "INFO:__main__:Processing prompt (16/31)...\n",
      "INFO:__main__:Processing prompt (17/31)...\n",
      "INFO:__main__:Processing prompt (18/31)...\n",
      "INFO:__main__:Processing prompt (19/31)...\n",
      "INFO:__main__:Processing prompt (20/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (2/31).\n",
      "INFO:__main__:Processing prompt (21/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (4/31).\n",
      "INFO:__main__:Processing prompt (22/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (12/31).\n",
      "INFO:__main__:Processing prompt (23/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (7/31).\n",
      "INFO:__main__:Processing prompt (24/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (16/31).\n",
      "INFO:__main__:Processing prompt (25/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (10/31).\n",
      "INFO:__main__:Processing prompt (26/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (8/31).\n",
      "INFO:__main__:Processing prompt (27/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (14/31).\n",
      "INFO:__main__:Processing prompt (28/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (3/31).\n",
      "INFO:__main__:Processing prompt (29/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (9/31).\n",
      "INFO:__main__:Processing prompt (30/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (17/31).\n",
      "INFO:__main__:Processing prompt (31/31)...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (19/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (6/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (5/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (13/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (11/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (18/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (15/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (20/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (22/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (1/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (23/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (28/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (21/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (24/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (26/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (30/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (25/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (31/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (27/31).\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processed prompt (29/31).\n",
      "INFO:__main__:Summarizing 31 batches...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:50025/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Summarized 31 batches and found 19 CSS selectors.\n",
      "INFO:__main__:Processing 31 pages...\n",
      "INFO:__main__:Processed 31 pages.\n",
      "INFO:__main__:Extracted 19 CSS selectors.\n"
     ]
    }
   ],
   "source": [
    "await extract_css_selectors(\"\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844086a",
   "metadata": {},
   "source": [
    "[] viewer should use save_state.json, maybe only work with save_state.json instead of all the other files? then have a result object that gets plugged into the save_state.json that can include extra data?\n",
    "- better method splitting, service classes, more models for example for counts of file extensions, frequent urls\n",
    "- one file with all kept urls and one with all not kept urls\n",
    "- say with what version we are working when starting sth, like v_000 (latest) or so"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
